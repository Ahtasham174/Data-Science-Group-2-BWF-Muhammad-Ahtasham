{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd90bdd2-c773-4f5d-99bb-e3b0ce608922",
   "metadata": {},
   "source": [
    "# Task 14: Data cleaning and preprocessing with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e8812-026f-4a56-9d51-006439fcde46",
   "metadata": {},
   "source": [
    "# Dataset 1 --> emissions_EU28.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234599b7-5bba-4f96-a035-95fc8622d87d",
   "metadata": {},
   "source": [
    "### Import Pandas and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67134059-6a2a-4018-9a38-1296988a33f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  airpol    cpa08 induse origin    unit geo\\time         2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        3.952        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  2027709.000  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     2027.709     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0.932        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   477994.000   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  \n",
      "0        3.965        3.955        4.025        4.082  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000  \n",
      "2     1996.301     1992.765     2023.324     2046.045  \n",
      "3        0.857        0.853        0.774        0.991  \n",
      "4   431637.000   429703.000   389072.000   496690.000  \n",
      "airpol       object\n",
      "cpa08        object\n",
      "induse       object\n",
      "origin       object\n",
      "unit         object\n",
      "geo\\time     object\n",
      "2018        float64\n",
      "2017        float64\n",
      "2016        float64\n",
      "2015        float64\n",
      "2014        float64\n",
      "2013        float64\n",
      "2012        float64\n",
      "2011        float64\n",
      "2010        float64\n",
      "2009        float64\n",
      "2008        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('emissions_EU28.csv')\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89886346-c8b4-4911-8422-c252d508ac78",
   "metadata": {},
   "source": [
    "### 1. Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344a2297-6093-45f9-8f21-3439be74032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "airpol      0\n",
      "cpa08       0\n",
      "induse      0\n",
      "origin      0\n",
      "unit        0\n",
      "geo\\time    0\n",
      "2018        0\n",
      "2017        0\n",
      "2016        0\n",
      "2015        0\n",
      "2014        0\n",
      "2013        0\n",
      "2012        0\n",
      "2011        0\n",
      "2010        0\n",
      "2009        0\n",
      "2008        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8f046-84cf-4ed6-aa8e-c5c8a91c084b",
   "metadata": {},
   "source": [
    "### 2. Drop Rows with Any Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82df5cc8-fdc0-4830-9169-4adaf544b1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping rows with missing values:\n",
      "  airpol    cpa08 induse origin    unit geo\\time         2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        3.952        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  2027709.000  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     2027.709     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0.932        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   477994.000   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  \n",
      "0        3.965        3.955        4.025        4.082  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000  \n",
      "2     1996.301     1992.765     2023.324     2046.045  \n",
      "3        0.857        0.853        0.774        0.991  \n",
      "4   431637.000   429703.000   389072.000   496690.000  \n"
     ]
    }
   ],
   "source": [
    "df_no_missing_rows = df.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with missing values:\")\n",
    "print(df_no_missing_rows.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f399449-e974-40b9-b36e-f753c0f0fec3",
   "metadata": {},
   "source": [
    "### 3. Drop Columns with Any Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d83e5f-8868-44f0-a37d-abf9c775ef98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping columns with missing values:\n",
      "  airpol    cpa08 induse origin    unit geo\\time         2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        3.952        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  2027709.000  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     2027.709     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0.932        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   477994.000   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  \n",
      "0        3.965        3.955        4.025        4.082  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000  \n",
      "2     1996.301     1992.765     2023.324     2046.045  \n",
      "3        0.857        0.853        0.774        0.991  \n",
      "4   431637.000   429703.000   389072.000   496690.000  \n"
     ]
    }
   ],
   "source": [
    "df_no_missing_cols = df.dropna(axis=1)\n",
    "print(\"\\nDataFrame after dropping columns with missing values:\")\n",
    "print(df_no_missing_cols.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe407ec-05a8-4689-9fa7-3cb48e341a42",
   "metadata": {},
   "source": [
    "### 4. Fill Missing Values with a Specific Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e245bb-ce7f-476b-b50f-3526a5b6ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after filling missing values with 0:\n",
      "  airpol    cpa08 induse origin    unit geo\\time         2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        3.952        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  2027709.000  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     2027.709     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0.932        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   477994.000   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  \n",
      "0        3.965        3.955        4.025        4.082  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000  \n",
      "2     1996.301     1992.765     2023.324     2046.045  \n",
      "3        0.857        0.853        0.774        0.991  \n",
      "4   431637.000   429703.000   389072.000   496690.000  \n"
     ]
    }
   ],
   "source": [
    "df_filled_value = df.fillna(0)\n",
    "print(\"\\nDataFrame after filling missing values with 0:\")\n",
    "print(df_filled_value.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626565e-3dde-4b9a-a0f4-9e4513e6cc34",
   "metadata": {},
   "source": [
    "### 5. Fill Missing Values Using Forward Fill and Backward Fill Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1668b924-0140-4228-9602-4d402dbfc783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Ahtasham\\AppData\\Local\\Temp\\ipykernel_20368\\2793431106.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_forward_fill = df.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after forward fill:\n",
      "  airpol    cpa08 induse origin    unit geo\\time         2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        3.952        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  2027709.000  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     2027.709     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0.932        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   477994.000   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  \n",
      "0        3.965        3.955        4.025        4.082  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000  \n",
      "2     1996.301     1992.765     2023.324     2046.045  \n",
      "3        0.857        0.853        0.774        0.991  \n",
      "4   431637.000   429703.000   389072.000   496690.000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Ahtasham\\AppData\\Local\\Temp\\ipykernel_20368\\2793431106.py:6: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_backward_fill = df.fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after backward fill:\n",
      "  airpol    cpa08 induse origin    unit geo\\time         2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        3.952        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  2027709.000  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     2027.709     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0.932        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   477994.000   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  \n",
      "0        3.965        3.955        4.025        4.082  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000  \n",
      "2     1996.301     1992.765     2023.324     2046.045  \n",
      "3        0.857        0.853        0.774        0.991  \n",
      "4   431637.000   429703.000   389072.000   496690.000  \n"
     ]
    }
   ],
   "source": [
    "df_forward_fill = df.fillna(method='ffill')\n",
    "print(\"\\nDataFrame after forward fill:\")\n",
    "print(df_forward_fill.head())\n",
    "\n",
    "# Backward fill missing values\n",
    "df_backward_fill = df.fillna(method='bfill')\n",
    "print(\"\\nDataFrame after backward fill:\")\n",
    "print(df_backward_fill.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029dabf7-9141-4a98-ac16-4a43c763a405",
   "metadata": {},
   "source": [
    "### 6. Interpolate Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21fdfaea-f777-4709-9435-764650bb3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after interpolating missing values:\n",
      "  airpol    cpa08 induse origin    unit geo\\time         2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        3.952        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  2027709.000  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     2027.709     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0.932        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   477994.000   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  \n",
      "0        3.965        3.955        4.025        4.082  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000  \n",
      "2     1996.301     1992.765     2023.324     2046.045  \n",
      "3        0.857        0.853        0.774        0.991  \n",
      "4   431637.000   429703.000   389072.000   496690.000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Ahtasham\\AppData\\Local\\Temp\\ipykernel_20368\\3783148640.py:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated = df.interpolate()\n"
     ]
    }
   ],
   "source": [
    "df_interpolated = df.interpolate()\n",
    "print(\"\\nDataFrame after interpolating missing values:\")\n",
    "print(df_interpolated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450656a-7162-4d0a-accc-44eb9bd28df8",
   "metadata": {},
   "source": [
    "### 7. Convert a Column to a Different Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29473cb-6fd3-412c-9175-a0eebc76ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after converting '2018' column to integer type:\n",
      "airpol       object\n",
      "cpa08        object\n",
      "induse       object\n",
      "origin       object\n",
      "unit         object\n",
      "geo\\time     object\n",
      "2018          int64\n",
      "2017        float64\n",
      "2016        float64\n",
      "2015        float64\n",
      "2014        float64\n",
      "2013        float64\n",
      "2012        float64\n",
      "2011        float64\n",
      "2010        float64\n",
      "2009        float64\n",
      "2008        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['2018'] = df['2018'].astype('int64')\n",
    "print(\"\\nDataFrame after converting '2018' column to integer type:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20408d-257b-4f8c-866a-ca9f165335f2",
   "metadata": {},
   "source": [
    "### 8. Apply a Function to Transform the Values of a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6432a2-27ad-4e4e-8f9c-ae1c4dcf0497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after applying a function to '2018' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  \n",
      "0        3.965        3.955        4.025        4.082  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000  \n",
      "2     1996.301     1992.765     2023.324     2046.045  \n",
      "3        0.857        0.853        0.774        0.991  \n",
      "4   431637.000   429703.000   389072.000   496690.000  \n"
     ]
    }
   ],
   "source": [
    "df['2018'] = df['2018'].apply(lambda x: x * 2)\n",
    "print(\"\\nDataFrame after applying a function to '2018' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25356ca-7159-4d46-9891-84179be65cdc",
   "metadata": {},
   "source": [
    "### 9. Normalize a Column Using Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ae01e1-6cf8-470c-ab76-3c0120840661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after Min-Max scaling of '2018' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  2018_normalized  \n",
      "0        3.965        3.955        4.025        4.082         0.002689  \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000         0.003056  \n",
      "2     1996.301     1992.765     2023.324     2046.045         0.002690  \n",
      "3        0.857        0.853        0.774        0.991         0.002689  \n",
      "4   431637.000   429703.000   389072.000   496690.000         0.002776  \n"
     ]
    }
   ],
   "source": [
    "df['2018_normalized'] = (df['2018'] - df['2018'].min()) / (df['2018'].max() - df['2018'].min())\n",
    "print(\"\\nDataFrame after Min-Max scaling of '2018' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68adb0be-16dc-45c8-a363-d97bf940e7c2",
   "metadata": {},
   "source": [
    "### 10. Standardize a Column (Z-Score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caccc5a9-b8ae-49ac-adb7-722e397a6739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after z-score normalization of '2018' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  2018_normalized  \\\n",
      "0        3.965        3.955        4.025        4.082         0.002689   \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000         0.003056   \n",
      "2     1996.301     1992.765     2023.324     2046.045         0.002690   \n",
      "3        0.857        0.853        0.774        0.991         0.002689   \n",
      "4   431637.000   429703.000   389072.000   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized  \n",
      "0          -0.033285  \n",
      "1           0.001351  \n",
      "2          -0.033251  \n",
      "3          -0.033285  \n",
      "4          -0.025120  \n"
     ]
    }
   ],
   "source": [
    "df['2018_standardized'] = (df['2018'] - df['2018'].mean()) / df['2018'].std()\n",
    "print(\"\\nDataFrame after z-score normalization of '2018' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0485fa-34fe-4c6a-a208-2eaf5c6f23b4",
   "metadata": {},
   "source": [
    "### 11. Identify Duplicate Rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cccca5b9-42cf-45d4-a081-e106ad90118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate rows in the DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [airpol, cpa08, induse, origin, unit, geo\\time, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2018_normalized, 2018_standardized]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicates = df.duplicated()\n",
    "print(\"\\nDuplicate rows in the DataFrame:\")\n",
    "print(df[duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b260b373-a77a-4b24-ad66-4da9d89ca489",
   "metadata": {},
   "source": [
    "### 12. Drop Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eab204c-98ac-4197-b5a4-0f0b9d25a077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping duplicate rows:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    ACG  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    ACG  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    ACG  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    ACG  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    ACG  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  2018_normalized  \\\n",
      "0        3.965        3.955        4.025        4.082         0.002689   \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000         0.003056   \n",
      "2     1996.301     1992.765     2023.324     2046.045         0.002690   \n",
      "3        0.857        0.853        0.774        0.991         0.002689   \n",
      "4   431637.000   429703.000   389072.000   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized  \n",
      "0          -0.033285  \n",
      "1           0.001351  \n",
      "2          -0.033251  \n",
      "3          -0.033285  \n",
      "4          -0.025120  \n"
     ]
    }
   ],
   "source": [
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nDataFrame after dropping duplicate rows:\")\n",
    "print(df_no_duplicates.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59743b3b-448e-4495-ad70-339458066687",
   "metadata": {},
   "source": [
    "### 13. Drop Duplicate Rows Based on Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "274fe1cc-02e3-484f-9562-6c664959cbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping duplicate rows based on 'airpol' and 'cpa08':\n",
      "    airpol       cpa08 induse origin    unit geo\\time  2018   2017   2016  \\\n",
      "0      ACG     CPA_A01     P3    DOM  KG_HAB     EU28     6  4.034  3.952   \n",
      "108    ACG     CPA_A02     P3    DOM  KG_HAB     EU28     0  0.019  0.018   \n",
      "216    ACG     CPA_A03     P3    DOM  KG_HAB     EU28     0  0.104  0.111   \n",
      "324    ACG       CPA_B     P3    DOM  KG_HAB     EU28     0  0.019  0.017   \n",
      "432    ACG  CPA_C10-12     P3    DOM  KG_HAB     EU28    10  5.429  6.062   \n",
      "\n",
      "      2015   2014   2013   2012   2011   2010   2009   2008  2018_normalized  \\\n",
      "0    3.901  3.824  4.182  4.260  3.965  3.955  4.025  4.082         0.002689   \n",
      "108  0.021  0.017  0.017  0.017  0.018  0.020  0.021  0.022         0.002689   \n",
      "216  0.088  0.097  0.108  0.108  0.111  0.130  0.137  0.130         0.002689   \n",
      "324  0.023  0.016  0.020  0.017  0.022  0.032  0.035  0.039         0.002689   \n",
      "432  5.752  5.873  6.135  6.249  6.170  6.442  6.808  6.603         0.002689   \n",
      "\n",
      "     2018_standardized  \n",
      "0            -0.033285  \n",
      "108          -0.033285  \n",
      "216          -0.033285  \n",
      "324          -0.033285  \n",
      "432          -0.033285  \n"
     ]
    }
   ],
   "source": [
    "df_no_dup_specific = df.drop_duplicates(subset=['airpol', 'cpa08'])\n",
    "print(\"\\nDataFrame after dropping duplicate rows based on 'airpol' and 'cpa08':\")\n",
    "print(df_no_dup_specific.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde84416-450d-4a31-9be5-126144afcb49",
   "metadata": {},
   "source": [
    "### 14. Convert All String Values in a Column to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b530d72e-0eb3-4d10-b259-85cbfc67fc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after converting 'airpol' column to lowercase:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  2018_normalized  \\\n",
      "0        3.965        3.955        4.025        4.082         0.002689   \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000         0.003056   \n",
      "2     1996.301     1992.765     2023.324     2046.045         0.002690   \n",
      "3        0.857        0.853        0.774        0.991         0.002689   \n",
      "4   431637.000   429703.000   389072.000   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized  \n",
      "0          -0.033285  \n",
      "1           0.001351  \n",
      "2          -0.033251  \n",
      "3          -0.033285  \n",
      "4          -0.025120  \n"
     ]
    }
   ],
   "source": [
    "df['airpol'] = df['airpol'].str.lower()\n",
    "print(\"\\nDataFrame after converting 'airpol' column to lowercase:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483aa99-f105-49d5-8070-f51f7aa3fed0",
   "metadata": {},
   "source": [
    "### 15. Remove Leading and Trailing Spaces from String Values in a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fb74cc4-826f-450d-b5a1-5c3ba977c88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after removing leading and trailing spaces from 'airpol' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  2018_normalized  \\\n",
      "0        3.965        3.955        4.025        4.082         0.002689   \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000         0.003056   \n",
      "2     1996.301     1992.765     2023.324     2046.045         0.002690   \n",
      "3        0.857        0.853        0.774        0.991         0.002689   \n",
      "4   431637.000   429703.000   389072.000   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized  \n",
      "0          -0.033285  \n",
      "1           0.001351  \n",
      "2          -0.033251  \n",
      "3          -0.033285  \n",
      "4          -0.025120  \n"
     ]
    }
   ],
   "source": [
    "df['airpol'] = df['airpol'].str.strip()\n",
    "print(\"\\nDataFrame after removing leading and trailing spaces from 'airpol' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2643f5f8-3056-479e-aec0-8d81177546be",
   "metadata": {},
   "source": [
    "### 16. Replace a Specific Substring in a Column with Another Substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f37667-a08b-401f-85e3-f96c041aab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after replacing 'co2' with 'CO2' in 'airpol' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  2018_normalized  \\\n",
      "0        3.965        3.955        4.025        4.082         0.002689   \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000         0.003056   \n",
      "2     1996.301     1992.765     2023.324     2046.045         0.002690   \n",
      "3        0.857        0.853        0.774        0.991         0.002689   \n",
      "4   431637.000   429703.000   389072.000   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized  \n",
      "0          -0.033285  \n",
      "1           0.001351  \n",
      "2          -0.033251  \n",
      "3          -0.033285  \n",
      "4          -0.025120  \n"
     ]
    }
   ],
   "source": [
    "df['airpol'] = df['airpol'].str.replace('co2', 'CO2')\n",
    "print(\"\\nDataFrame after replacing 'co2' with 'CO2' in 'airpol' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2d49d-ff75-4044-afda-6330a7b3745c",
   "metadata": {},
   "source": [
    "### 17. Extract a Substring from Each Value in a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2e731ba-951b-4a25-a9a8-4026fa09ed6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after extracting first 3 characters of 'airpol' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015         2014         2013         2012  \\\n",
      "0        3.952        3.901        3.824        4.182        4.260   \n",
      "1  2018821.000  1986991.000  1942285.000  2116310.000  2149511.000   \n",
      "2     2018.821     1986.991     1942.285     2116.310     2149.511   \n",
      "3        0.862        0.898        0.732        0.706        0.762   \n",
      "4   440390.000   457340.000   371967.000   357358.000   384481.000   \n",
      "\n",
      "          2011         2010         2009         2008  2018_normalized  \\\n",
      "0        3.965        3.955        4.025        4.082         0.002689   \n",
      "1  1996301.000  1992765.000  2023324.000  2046045.000         0.003056   \n",
      "2     1996.301     1992.765     2023.324     2046.045         0.002690   \n",
      "3        0.857        0.853        0.774        0.991         0.002689   \n",
      "4   431637.000   429703.000   389072.000   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized airpol_substring  \n",
      "0          -0.033285              acg  \n",
      "1           0.001351              acg  \n",
      "2          -0.033251              acg  \n",
      "3          -0.033285              acg  \n",
      "4          -0.025120              acg  \n"
     ]
    }
   ],
   "source": [
    "df['airpol_substring'] = df['airpol'].str[:3]\n",
    "print(\"\\nDataFrame after extracting first 3 characters of 'airpol' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a9a38-cf2f-4a73-9927-00fcb0a6cf49",
   "metadata": {},
   "source": [
    "### 18. Convert a Column to Datetime Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e4d05b3-edb0-43f1-8416-299b04ddde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after adding 'date' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015  ...         2013         2012         2011  \\\n",
      "0        3.952        3.901  ...        4.182        4.260        3.965   \n",
      "1  2018821.000  1986991.000  ...  2116310.000  2149511.000  1996301.000   \n",
      "2     2018.821     1986.991  ...     2116.310     2149.511     1996.301   \n",
      "3        0.862        0.898  ...        0.706        0.762        0.857   \n",
      "4   440390.000   457340.000  ...   357358.000   384481.000   431637.000   \n",
      "\n",
      "          2010         2009         2008  2018_normalized  2018_standardized  \\\n",
      "0        3.955        4.025        4.082         0.002689          -0.033285   \n",
      "1  1992765.000  2023324.000  2046045.000         0.003056           0.001351   \n",
      "2     1992.765     2023.324     2046.045         0.002690          -0.033251   \n",
      "3        0.853        0.774        0.991         0.002689          -0.033285   \n",
      "4   429703.000   389072.000   496690.000         0.002776          -0.025120   \n",
      "\n",
      "   airpol_substring       date  \n",
      "0               acg 2020-01-01  \n",
      "1               acg 2020-01-01  \n",
      "2               acg 2020-01-01  \n",
      "3               acg 2020-01-01  \n",
      "4               acg 2020-01-01  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df['date'] = pd.to_datetime('2020-01-01')\n",
    "print(\"\\nDataFrame after adding 'date' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10401f92-1634-43c3-849c-1c1530e35d65",
   "metadata": {},
   "source": [
    "### 19. Extract Year, Month, and Day from a Datetime Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e546bdce-f309-4717-8536-f872199c5324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after extracting year, month, and day from 'date' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015  ...         2010         2009         2008  \\\n",
      "0        3.952        3.901  ...        3.955        4.025        4.082   \n",
      "1  2018821.000  1986991.000  ...  1992765.000  2023324.000  2046045.000   \n",
      "2     2018.821     1986.991  ...     1992.765     2023.324     2046.045   \n",
      "3        0.862        0.898  ...        0.853        0.774        0.991   \n",
      "4   440390.000   457340.000  ...   429703.000   389072.000   496690.000   \n",
      "\n",
      "   2018_normalized  2018_standardized  airpol_substring       date  year  \\\n",
      "0         0.002689          -0.033285               acg 2020-01-01  2020   \n",
      "1         0.003056           0.001351               acg 2020-01-01  2020   \n",
      "2         0.002690          -0.033251               acg 2020-01-01  2020   \n",
      "3         0.002689          -0.033285               acg 2020-01-01  2020   \n",
      "4         0.002776          -0.025120               acg 2020-01-01  2020   \n",
      "\n",
      "   month day  \n",
      "0      1   1  \n",
      "1      1   1  \n",
      "2      1   1  \n",
      "3      1   1  \n",
      "4      1   1  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "print(\"\\nDataFrame after extracting year, month, and day from 'date' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9b623-cebb-444e-b232-0397f150280b",
   "metadata": {},
   "source": [
    "### 20. Filter Rows Based on a Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91748566-1ac6-45fc-b442-8697f4b499a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after filtering rows based on date range 2010-2018:\n",
      "Empty DataFrame\n",
      "Columns: [airpol, cpa08, induse, origin, unit, geo\\time, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2018_normalized, 2018_standardized, airpol_substring, date, year, month, day]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df_date_filtered = df[(df['date'] >= '2010-01-01') & (df['date'] <= '2018-12-31')]\n",
    "print(\"\\nDataFrame after filtering rows based on date range 2010-2018:\")\n",
    "print(df_date_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eca131-c580-407a-a816-d3fd5c2a5c90",
   "metadata": {},
   "source": [
    "### 21. Convert a Categorical Column to Numerical Using One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcb12bc3-cfc8-42be-8460-1374fc11337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after one-hot encoding 'airpol' column:\n",
      "     cpa08 induse origin    unit geo\\time     2018         2017         2016  \\\n",
      "0  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034        3.952   \n",
      "1  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000  2018821.000   \n",
      "2  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165     2018.821   \n",
      "3  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930        0.862   \n",
      "4  CPA_A01     P3    ROW       T     EU28   955988   476142.000   440390.000   \n",
      "\n",
      "          2015         2014  ...  airpol_nh3_so2e  airpol_nmvoc  airpol_nox  \\\n",
      "0        3.901        3.824  ...            False         False       False   \n",
      "1  1986991.000  1942285.000  ...            False         False       False   \n",
      "2     1986.991     1942.285  ...            False         False       False   \n",
      "3        0.898        0.732  ...            False         False       False   \n",
      "4   457340.000   371967.000  ...            False         False       False   \n",
      "\n",
      "   airpol_nox_nmvoce  airpol_nox_so2e  airpol_o3pr  airpol_pfc_CO2e  \\\n",
      "0              False            False        False            False   \n",
      "1              False            False        False            False   \n",
      "2              False            False        False            False   \n",
      "3              False            False        False            False   \n",
      "4              False            False        False            False   \n",
      "\n",
      "   airpol_pm10 airpol_pm2_5 airpol_sox_so2e  \n",
      "0        False        False           False  \n",
      "1        False        False           False  \n",
      "2        False        False           False  \n",
      "3        False        False           False  \n",
      "4        False        False           False  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "df_one_hot_encoded = pd.get_dummies(df, columns=['airpol'])\n",
    "print(\"\\nDataFrame after one-hot encoding 'airpol' column:\")\n",
    "print(df_one_hot_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f340bea-aa52-4cde-92e6-4282dea818cb",
   "metadata": {},
   "source": [
    "### 22. Convert a Categorical Column to Numerical Using Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1b76af8-cfb5-442c-b969-6dcf0cbd7875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after label encoding 'airpol' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015  ...         2009         2008  2018_normalized  \\\n",
      "0        3.952        3.901  ...        4.025        4.082         0.002689   \n",
      "1  2018821.000  1986991.000  ...  2023324.000  2046045.000         0.003056   \n",
      "2     2018.821     1986.991  ...     2023.324     2046.045         0.002690   \n",
      "3        0.862        0.898  ...        0.774        0.991         0.002689   \n",
      "4   440390.000   457340.000  ...   389072.000   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized  airpol_substring       date  year  month  day  \\\n",
      "0          -0.033285               acg 2020-01-01  2020      1    1   \n",
      "1           0.001351               acg 2020-01-01  2020      1    1   \n",
      "2          -0.033251               acg 2020-01-01  2020      1    1   \n",
      "3          -0.033285               acg 2020-01-01  2020      1    1   \n",
      "4          -0.025120               acg 2020-01-01  2020      1    1   \n",
      "\n",
      "  airpol_encoded  \n",
      "0              2  \n",
      "1              2  \n",
      "2              2  \n",
      "3              2  \n",
      "4              2  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "df['airpol_encoded'] = df['airpol'].astype('category').cat.codes\n",
    "print(\"\\nDataFrame after label encoding 'airpol' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce9cd2-cdcd-4ce4-a10a-bcb50d42a153",
   "metadata": {},
   "source": [
    "### 23. Group Values in a Categorical Column and Create a New Column with Grouped Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2baf1a7-a995-4e38-b023-bebddaa2f8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after grouping 'airpol' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015  ...         2008  2018_normalized  \\\n",
      "0        3.952        3.901  ...        4.082         0.002689   \n",
      "1  2018821.000  1986991.000  ...  2046045.000         0.003056   \n",
      "2     2018.821     1986.991  ...     2046.045         0.002690   \n",
      "3        0.862        0.898  ...        0.991         0.002689   \n",
      "4   440390.000   457340.000  ...   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized  airpol_substring       date  year  month  day  \\\n",
      "0          -0.033285               acg 2020-01-01  2020      1    1   \n",
      "1           0.001351               acg 2020-01-01  2020      1    1   \n",
      "2          -0.033251               acg 2020-01-01  2020      1    1   \n",
      "3          -0.033285               acg 2020-01-01  2020      1    1   \n",
      "4          -0.025120               acg 2020-01-01  2020      1    1   \n",
      "\n",
      "   airpol_encoded airpol_grouped  \n",
      "0               2         group2  \n",
      "1               2         group2  \n",
      "2               2         group2  \n",
      "3               2         group2  \n",
      "4               2         group2  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "def group_airpol(x):\n",
    "    if x in ['co2', 'ch4']:\n",
    "        return 'group1'\n",
    "    else:\n",
    "        return 'group2'\n",
    "\n",
    "df['airpol_grouped'] = df['airpol'].apply(group_airpol)\n",
    "print(\"\\nDataFrame after grouping 'airpol' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa99445-4d11-4cad-8e6a-e38e440f8c54",
   "metadata": {},
   "source": [
    "### 24. Merge Two DataFrames Based on a Common Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "082cba58-d79c-4ef0-8e5d-91c56683628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrame based on 'airpol' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    ch4  CPA_A01     P3    DOM  KG_HAB     EU28        8        5.053   \n",
      "1    ch4  CPA_A01     P3    DOM       T     EU28  5113510  2586624.000   \n",
      "2    ch4  CPA_A01     P3    DOM   THS_T     EU28     5112     2586.624   \n",
      "3    ch4  CPA_A01     P3    ROW  KG_HAB     EU28        2        1.200   \n",
      "4    ch4  CPA_A01     P3    ROW       T     EU28  1238778   614328.000   \n",
      "\n",
      "          2016         2015  ...  2018_normalized  2018_standardized  \\\n",
      "0        4.948        4.888  ...         0.002689          -0.033285   \n",
      "1  2527350.000  2489581.000  ...         0.003151           0.010388   \n",
      "2     2527.350     2489.581  ...         0.002690          -0.033242   \n",
      "3        1.110        1.173  ...         0.002689          -0.033285   \n",
      "4   566745.000   597375.000  ...         0.002801          -0.022705   \n",
      "\n",
      "   airpol_substring       date  year  month  day  airpol_encoded  \\\n",
      "0               ch4 2020-01-01  2020      1    1               3   \n",
      "1               ch4 2020-01-01  2020      1    1               3   \n",
      "2               ch4 2020-01-01  2020      1    1               3   \n",
      "3               ch4 2020-01-01  2020      1    1               3   \n",
      "4               ch4 2020-01-01  2020      1    1               3   \n",
      "\n",
      "   airpol_grouped description  \n",
      "0          group1     Methane  \n",
      "1          group1     Methane  \n",
      "2          group1     Methane  \n",
      "3          group1     Methane  \n",
      "4          group1     Methane  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'airpol': ['co2', 'ch4', 'n2o', 'co', 'nh3'],\n",
    "    'description': ['Carbon Dioxide', 'Methane', 'Nitrous Oxide', 'Carbon Monoxide', 'Ammonia']\n",
    "})\n",
    "\n",
    "df_merged = pd.merge(df, df2, on='airpol')\n",
    "print(\"\\nMerged DataFrame based on 'airpol' column:\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb1039-c003-4547-b3c7-a046b5227d62",
   "metadata": {},
   "source": [
    "### 25. Concatenate Two DataFrames Vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec473470-2c13-4627-88f6-afeb7ab60203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after vertical concatenation:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015  ...         2008  2018_normalized  \\\n",
      "0        3.952        3.901  ...        4.082         0.002689   \n",
      "1  2018821.000  1986991.000  ...  2046045.000         0.003056   \n",
      "2     2018.821     1986.991  ...     2046.045         0.002690   \n",
      "3        0.862        0.898  ...        0.991         0.002689   \n",
      "4   440390.000   457340.000  ...   496690.000         0.002776   \n",
      "\n",
      "   2018_standardized  airpol_substring       date  year  month  day  \\\n",
      "0          -0.033285               acg 2020-01-01  2020      1    1   \n",
      "1           0.001351               acg 2020-01-01  2020      1    1   \n",
      "2          -0.033251               acg 2020-01-01  2020      1    1   \n",
      "3          -0.033285               acg 2020-01-01  2020      1    1   \n",
      "4          -0.025120               acg 2020-01-01  2020      1    1   \n",
      "\n",
      "   airpol_encoded airpol_grouped  \n",
      "0               2         group2  \n",
      "1               2         group2  \n",
      "2               2         group2  \n",
      "3               2         group2  \n",
      "4               2         group2  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "df_concat_vert = pd.concat([df, df], axis=0)\n",
    "print(\"\\nDataFrame after vertical concatenation:\")\n",
    "print(df_concat_vert.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d290b0c-d24a-42ad-833c-b2a4d266d059",
   "metadata": {},
   "source": [
    "### 26. Concatenate Two DataFrames Horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea153658-dfd6-4692-b0ca-f54d9b072c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after horizontal concatenation:\n",
      "   airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0     acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1     acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2     acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3     acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4     acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015  ...  2018_standardized  airpol_substring  \\\n",
      "0        3.952        3.901  ...          -0.033285               acg   \n",
      "1  2018821.000  1986991.000  ...           0.001351               acg   \n",
      "2     2018.821     1986.991  ...          -0.033251               acg   \n",
      "3        0.862        0.898  ...          -0.033285               acg   \n",
      "4   440390.000   457340.000  ...          -0.025120               acg   \n",
      "\n",
      "        date  year  month  day  airpol_encoded  airpol_grouped  airpol  \\\n",
      "0 2020-01-01  2020      1    1               2          group2     co2   \n",
      "1 2020-01-01  2020      1    1               2          group2     ch4   \n",
      "2 2020-01-01  2020      1    1               2          group2     n2o   \n",
      "3 2020-01-01  2020      1    1               2          group2      co   \n",
      "4 2020-01-01  2020      1    1               2          group2     nh3   \n",
      "\n",
      "       description  \n",
      "0   Carbon Dioxide  \n",
      "1          Methane  \n",
      "2    Nitrous Oxide  \n",
      "3  Carbon Monoxide  \n",
      "4          Ammonia  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "df_concat_horiz = pd.concat([df, df2], axis=1)\n",
    "print(\"\\nDataFrame after horizontal concatenation:\")\n",
    "print(df_concat_horiz.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b3cc5-ee39-4328-873e-576c2226ba9c",
   "metadata": {},
   "source": [
    "### 27. Create a New Column Based on Existing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fe1cfb9-3533-4fe7-ab8d-e038e5ca01da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after creating 'total_emissions' column:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015  ...  2018_normalized  2018_standardized  \\\n",
      "0        3.952        3.901  ...         0.002689          -0.033285   \n",
      "1  2018821.000  1986991.000  ...         0.003056           0.001351   \n",
      "2     2018.821     1986.991  ...         0.002690          -0.033251   \n",
      "3        0.862        0.898  ...         0.002689          -0.033285   \n",
      "4   440390.000   457340.000  ...         0.002776          -0.025120   \n",
      "\n",
      "   airpol_substring       date  year  month  day  airpol_encoded  \\\n",
      "0               acg 2020-01-01  2020      1    1               2   \n",
      "1               acg 2020-01-01  2020      1    1               2   \n",
      "2               acg 2020-01-01  2020      1    1               2   \n",
      "3               acg 2020-01-01  2020      1    1               2   \n",
      "4               acg 2020-01-01  2020      1    1               2   \n",
      "\n",
      "   airpol_grouped total_emissions  \n",
      "0          group2    2.171100e+01  \n",
      "1          group2    1.206868e+07  \n",
      "2          group2    1.206726e+04  \n",
      "3          group2    3.422000e+00  \n",
      "4          group2    2.701827e+06  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df['total_emissions'] = df[['2018', '2017', '2016', '2015', '2014']].sum(axis=1)\n",
    "print(\"\\nDataFrame after creating 'total_emissions' column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc588b5-70f7-4258-a921-f309469506af",
   "metadata": {},
   "source": [
    "### 28. Discretize a Continuous Column into Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47cc2dd0-023f-46b4-9a4e-36ebbac896ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after discretizing '2018' column into bins:\n",
      "  airpol    cpa08 induse origin    unit geo\\time     2018         2017  \\\n",
      "0    acg  CPA_A01     P3    DOM  KG_HAB     EU28        6        4.034   \n",
      "1    acg  CPA_A01     P3    DOM       T     EU28  4055418  2065165.000   \n",
      "2    acg  CPA_A01     P3    DOM   THS_T     EU28     4054     2065.165   \n",
      "3    acg  CPA_A01     P3    ROW  KG_HAB     EU28        0        0.930   \n",
      "4    acg  CPA_A01     P3    ROW       T     EU28   955988   476142.000   \n",
      "\n",
      "          2016         2015  ...  2018_standardized  airpol_substring  \\\n",
      "0        3.952        3.901  ...          -0.033285               acg   \n",
      "1  2018821.000  1986991.000  ...           0.001351               acg   \n",
      "2     2018.821     1986.991  ...          -0.033251               acg   \n",
      "3        0.862        0.898  ...          -0.033285               acg   \n",
      "4   440390.000   457340.000  ...          -0.025120               acg   \n",
      "\n",
      "        date  year  month  day  airpol_encoded  airpol_grouped  \\\n",
      "0 2020-01-01  2020      1    1               2          group2   \n",
      "1 2020-01-01  2020      1    1               2          group2   \n",
      "2 2020-01-01  2020      1    1               2          group2   \n",
      "3 2020-01-01  2020      1    1               2          group2   \n",
      "4 2020-01-01  2020      1    1               2          group2   \n",
      "\n",
      "   total_emissions emission_bins  \n",
      "0     2.171100e+01           Low  \n",
      "1     1.206868e+07           Low  \n",
      "2     1.206726e+04           Low  \n",
      "3     3.422000e+00           Low  \n",
      "4     2.701827e+06           Low  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "df['emission_bins'] = pd.cut(df['2018'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "print(\"\\nDataFrame after discretizing '2018' column into bins:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981bc125-9368-4c97-ba75-f54f6bba8031",
   "metadata": {},
   "source": [
    "### 29. Create Polynomial Features from Existing Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28f9a0e0-e5f7-494e-91ea-70cd32575633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with polynomial features:\n",
      "     1       2018         2017        2018^2     2018*2017        2017^2\n",
      "0  1.0        6.0        4.034  3.600000e+01  2.420400e+01  1.627316e+01\n",
      "1  1.0  4055418.0  2065165.000  1.644642e+13  8.375107e+12  4.264906e+12\n",
      "2  1.0     4054.0     2065.165  1.643492e+07  8.372179e+06  4.264906e+06\n",
      "3  1.0        0.0        0.930  0.000000e+00  0.000000e+00  8.649000e-01\n",
      "4  1.0   955988.0   476142.000  9.139131e+11  4.551860e+11  2.267112e+11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly_features = poly.fit_transform(df[['2018', '2017']])\n",
    "df_poly = pd.DataFrame(poly_features, columns=['1', '2018', '2017', '2018^2', '2018*2017', '2017^2'])\n",
    "print(\"\\nDataFrame with polynomial features:\")\n",
    "print(df_poly.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ac1c9-8afc-48ce-81da-4f21bac0255b",
   "metadata": {},
   "source": [
    "# Dataset 2 --> final_book_dataset_kaggle2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8368ea-2c49-4b4e-b13e-48edc20079a5",
   "metadata": {},
   "source": [
    "### Import Pandas and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3507bb63-b97b-4ee8-a4de-47616b93adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n",
      "                                               title  \\\n",
      "0  Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1  Head First Data Analysis: A learner's guide to...   \n",
      "2  Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3  Python for Data Analysis: Data Wrangling with ...   \n",
      "4  Excel Data Analysis For Dummies (For Dummies (...   \n",
      "\n",
      "                               author  price price (including used books)  \\\n",
      "0                 [ Dr Dhaval Maheta]   6.75                         6.75   \n",
      "1                                 NaN  33.72               21.49 - 33.72    \n",
      "2  [ Oz du Soleil,  and , Bill Jelen]  32.07                        32.07   \n",
      "3                 [ William McKinney]  53.99                        53.99   \n",
      "4                   [ Paul McFedries]  24.49                        24.49   \n",
      "\n",
      "  pages  avg_reviews n_reviews star5 star4 star3 star2 star1  \\\n",
      "0   500          4.4        23   55%   39%    6%   NaN   NaN   \n",
      "1   484          4.3       124   61%   20%    9%    4%    6%   \n",
      "2   274          4.7        10   87%   13%   NaN   NaN   NaN   \n",
      "3   547          4.6     1,686   75%   16%    5%    2%    2%   \n",
      "4   368          3.9        12   52%   17%   10%   10%   10%   \n",
      "\n",
      "                  dimensions       weight language  \\\n",
      "0     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
      "1     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
      "2  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
      "3     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "\n",
      "                                           publisher         ISBN_13  \\\n",
      "0     Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1      O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2  Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3    O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4        For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "\n",
      "                                                link  \\\n",
      "0  /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1  /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2  /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3  /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4  /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "\n",
      "                                       complete_link  \n",
      "0  https://www.amazon.com/Data-Analysis-Using-Low...  \n",
      "1  https://www.amazon.com/Head-First-Data-Analysi...  \n",
      "2  https://www.amazon.com/Guerrilla-Analysis-Usin...  \n",
      "3  https://www.amazon.com/Python-Data-Analysis-Wr...  \n",
      "4  https://www.amazon.com/Excel-Data-Analysis-Dum...  \n",
      "title                            object\n",
      "author                           object\n",
      "price                           float64\n",
      "price (including used books)     object\n",
      "pages                            object\n",
      "avg_reviews                     float64\n",
      "n_reviews                        object\n",
      "star5                            object\n",
      "star4                            object\n",
      "star3                            object\n",
      "star2                            object\n",
      "star1                            object\n",
      "dimensions                       object\n",
      "weight                           object\n",
      "language                         object\n",
      "publisher                        object\n",
      "ISBN_13                          object\n",
      "link                             object\n",
      "complete_link                    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('final_book_dataset_kaggle2.csv')\n",
    "print(\"Initial DataFrame:\")\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efee582-6019-4dfd-90b2-4ef7caf3b1da",
   "metadata": {},
   "source": [
    "### 1. Identify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c91c187c-ffd1-4d23-b05d-6d9e494e933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "title                             0\n",
      "author                          173\n",
      "price                           108\n",
      "price (including used books)    108\n",
      "pages                            85\n",
      "avg_reviews                     128\n",
      "n_reviews                       128\n",
      "star5                           128\n",
      "star4                           195\n",
      "star3                           276\n",
      "star2                           379\n",
      "star1                           502\n",
      "dimensions                      186\n",
      "weight                          179\n",
      "language                         71\n",
      "publisher                       116\n",
      "ISBN_13                         165\n",
      "link                              0\n",
      "complete_link                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b4764-3774-4946-9650-42ebb3a081f3",
   "metadata": {},
   "source": [
    "### 2. Drop Rows with Any Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1621f46-b384-4038-9ea1-05032a4b3b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping rows with missing values:\n",
      "                                                 title  \\\n",
      "3    Python for Data Analysis: Data Wrangling with ...   \n",
      "4    Excel Data Analysis For Dummies (For Dummies (...   \n",
      "6    SQL for Data Analysis: Advanced Techniques for...   \n",
      "15   SQL QuickStart Guide: The Simplified Beginner'...   \n",
      "18   Python para Principiantes: 2 Libros en 1: Prog...   \n",
      "..                                                 ...   \n",
      "804  Essential Calculus Skills Practice Workbook wi...   \n",
      "807  Machine Learning with R: Expert techniques for...   \n",
      "821  Machine Learning with PyTorch and Scikit-Learn...   \n",
      "825   Deep Learning: Engage the World Change the World   \n",
      "826  Machine Learning in Finance: From Theory to Pr...   \n",
      "\n",
      "                                              author  price  \\\n",
      "3                                [ William McKinney]  53.99   \n",
      "4                                  [ Paul McFedries]  24.49   \n",
      "6                                  [ Cathy Tanimura]  40.49   \n",
      "15                                 [ Walter Shields]  24.99   \n",
      "18                  [ Programming Languages Academy]  19.38   \n",
      "..                                               ...    ...   \n",
      "804                                [ Chris McMullen]   9.99   \n",
      "807                                   [ Brett Lantz]  37.99   \n",
      "821  [ Sebastian Raschka, Yuxi (Hayden) Liu, et al.]  39.02   \n",
      "825          [ Michael Fullan, Joanne Quinn, et al.]   5.33   \n",
      "826       [ Matthew F. Dixon, Igor Halperin, et al.]  55.18   \n",
      "\n",
      "    price (including used books) pages  avg_reviews n_reviews star5 star4  \\\n",
      "3                          53.99   547          4.6     1,686   75%   16%   \n",
      "4                          24.49   368          3.9        12   52%   17%   \n",
      "6                          40.49   360          4.6        72   75%   18%   \n",
      "15                         24.99   249          4.6     1,358   72%   18%   \n",
      "18                         19.38   234          4.4        56   73%    8%   \n",
      "..                           ...   ...          ...       ...   ...   ...   \n",
      "804                         9.99   151          4.6     1,537   74%   16%   \n",
      "807               24.96 - 37.99    458          4.6       237   78%   13%   \n",
      "821                        39.02   774          4.6       164   80%   11%   \n",
      "825                8.55 - 35.33    208          4.5        74   72%   14%   \n",
      "826               52.41 - 55.18    573          4.5        93   78%    7%   \n",
      "\n",
      "    star3 star2 star1                 dimensions       weight language  \\\n",
      "3      5%    2%    2%     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4     10%   10%   10%  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "6      2%    2%    2%  6.75 x 0.75 x 8.75 inches   1.2 pounds  English   \n",
      "15     7%    1%    2%   7.5 x 0.57 x 9.25 inches  15.5 ounces  English   \n",
      "18    10%    4%    4%        6 x 0.53 x 9 inches  11.4 ounces  Spanish   \n",
      "..    ...   ...   ...                        ...          ...      ...   \n",
      "804    7%    1%    2%       8 x 0.35 x 10 inches    11 ounces  English   \n",
      "807    5%    2%    2%   7.5 x 1.04 x 9.25 inches  1.72 pounds  English   \n",
      "821    4%    3%    3%   7.5 x 1.75 x 9.25 inches  2.88 pounds  English   \n",
      "825    7%    2%    4%       7 x 0.47 x 10 inches  15.5 ounces  English   \n",
      "826    7%    4%    4%  6.14 x 1.25 x 9.21 inches  2.25 pounds  English   \n",
      "\n",
      "                                             publisher         ISBN_13  \\\n",
      "3      O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4          For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "6        O'Reilly Media; 1st edition (October 5, 2021)  978-1492088783   \n",
      "15   ClydeBank Media LLC; Illustrated edition (Nove...  978-1945051753   \n",
      "18               Independently published (May 2, 2020)  979-8642649534   \n",
      "..                                                 ...             ...   \n",
      "804                Zishka Publishing (August 16, 2018)  978-1941691243   \n",
      "807     Packt Publishing; 3rd edition (April 15, 2019)                   \n",
      "821               Packt Publishing (February 25, 2022)  978-1801819312   \n",
      "825          Corwin; First edition (December 15, 2017)              59   \n",
      "826      Springer; 1st ed. 2020 edition (July 2, 2020)                   \n",
      "\n",
      "                                                  link  \\\n",
      "3    /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4    /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "6    /SQL-Data-Analysis-Techniques-Transforming/dp/...   \n",
      "15   /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "18   /Python-para-Principiantes-Programaci%C3%B3n-p...   \n",
      "..                                                 ...   \n",
      "804  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "807  /Machine-Learning-techniques-predictive-modeli...   \n",
      "821  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "825  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "826  /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "\n",
      "                                         complete_link  \n",
      "3    https://www.amazon.com/Python-Data-Analysis-Wr...  \n",
      "4    https://www.amazon.com/Excel-Data-Analysis-Dum...  \n",
      "6    https://www.amazon.com/SQL-Data-Analysis-Techn...  \n",
      "15   https://www.amazon.com/gp/slredirect/picassoRe...  \n",
      "18   https://www.amazon.com/Python-para-Principiant...  \n",
      "..                                                 ...  \n",
      "804  https://www.amazon.com/gp/slredirect/picassoRe...  \n",
      "807  https://www.amazon.com/Machine-Learning-techni...  \n",
      "821  https://www.amazon.com/gp/slredirect/picassoRe...  \n",
      "825  https://www.amazon.com/Deep-Learning-Engage-Wo...  \n",
      "826  https://www.amazon.com/Machine-Learning-Financ...  \n",
      "\n",
      "[223 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dropped_rows = df.dropna()\n",
    "print(\"\\nDataFrame after dropping rows with missing values:\")\n",
    "print(df_dropped_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a020b0-d43d-44b7-8a00-0d3ecd3e3306",
   "metadata": {},
   "source": [
    "### 3. Drop Rows with Any Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10bbb90d-3136-4ceb-a481-82bf7b3cb4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping columns with missing values:\n",
      "                                                 title  \\\n",
      "0    Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1    Head First Data Analysis: A learner's guide to...   \n",
      "2    Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3    Python for Data Analysis: Data Wrangling with ...   \n",
      "4    Excel Data Analysis For Dummies (For Dummies (...   \n",
      "..                                                 ...   \n",
      "825   Deep Learning: Engage the World Change the World   \n",
      "826  Machine Learning in Finance: From Theory to Pr...   \n",
      "827  Practical Deep Learning at Scale with MLflow: ...   \n",
      "828  Clinical Biostatistics and Epidemiology Made R...   \n",
      "829  AI and Machine Learning for Coders: A Programm...   \n",
      "\n",
      "                                                  link  \\\n",
      "0    /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1    /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2    /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3    /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4    /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "..                                                 ...   \n",
      "825  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "826  /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "827  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "828  /Clinical-Biostatistics-Epidemiology-Ridiculou...   \n",
      "829  /Machine-Learning-Coders-Programmers-Intellige...   \n",
      "\n",
      "                                         complete_link  \n",
      "0    https://www.amazon.com/Data-Analysis-Using-Low...  \n",
      "1    https://www.amazon.com/Head-First-Data-Analysi...  \n",
      "2    https://www.amazon.com/Guerrilla-Analysis-Usin...  \n",
      "3    https://www.amazon.com/Python-Data-Analysis-Wr...  \n",
      "4    https://www.amazon.com/Excel-Data-Analysis-Dum...  \n",
      "..                                                 ...  \n",
      "825  https://www.amazon.com/Deep-Learning-Engage-Wo...  \n",
      "826  https://www.amazon.com/Machine-Learning-Financ...  \n",
      "827  https://www.amazon.com/gp/slredirect/picassoRe...  \n",
      "828  https://www.amazon.com/Clinical-Biostatistics-...  \n",
      "829  https://www.amazon.com/Machine-Learning-Coders...  \n",
      "\n",
      "[830 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dropped_columns = df.dropna(axis=1)\n",
    "print(\"\\nDataFrame after dropping columns with missing values:\")\n",
    "print(df_dropped_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034fcb88-1562-42e4-9115-b5538f5edf3c",
   "metadata": {},
   "source": [
    "### 4. Drop Columns with Any Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "083a197d-af90-41a4-b738-1041002fcadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after filling missing values with 0:\n",
      "                                                 title  \\\n",
      "0    Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1    Head First Data Analysis: A learner's guide to...   \n",
      "2    Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3    Python for Data Analysis: Data Wrangling with ...   \n",
      "4    Excel Data Analysis For Dummies (For Dummies (...   \n",
      "..                                                 ...   \n",
      "825   Deep Learning: Engage the World Change the World   \n",
      "826  Machine Learning in Finance: From Theory to Pr...   \n",
      "827  Practical Deep Learning at Scale with MLflow: ...   \n",
      "828  Clinical Biostatistics and Epidemiology Made R...   \n",
      "829  AI and Machine Learning for Coders: A Programm...   \n",
      "\n",
      "                                         author  price  \\\n",
      "0                           [ Dr Dhaval Maheta]   6.75   \n",
      "1                                             0  33.72   \n",
      "2            [ Oz du Soleil,  and , Bill Jelen]  32.07   \n",
      "3                           [ William McKinney]  53.99   \n",
      "4                             [ Paul McFedries]  24.49   \n",
      "..                                          ...    ...   \n",
      "825     [ Michael Fullan, Joanne Quinn, et al.]   5.33   \n",
      "826  [ Matthew F. Dixon, Igor Halperin, et al.]  55.18   \n",
      "827       [ Yong Liu,  and , Dr. Matei Zaharia]  44.99   \n",
      "828                                           0   0.00   \n",
      "829                         [ Laurence Moroney]  38.49   \n",
      "\n",
      "    price (including used books) pages  avg_reviews n_reviews star5 star4  \\\n",
      "0                           6.75   500          4.4        23   55%   39%   \n",
      "1                 21.49 - 33.72    484          4.3       124   61%   20%   \n",
      "2                          32.07   274          4.7        10   87%   13%   \n",
      "3                          53.99   547          4.6     1,686   75%   16%   \n",
      "4                          24.49   368          3.9        12   52%   17%   \n",
      "..                           ...   ...          ...       ...   ...   ...   \n",
      "825                8.55 - 35.33    208          4.5        74   72%   14%   \n",
      "826               52.41 - 55.18    573          4.5        93   78%    7%   \n",
      "827                        44.99   288          4.8         8   83%   17%   \n",
      "828                            0     0          0.0         0     0     0   \n",
      "829                        38.49     0          4.7       142   79%   15%   \n",
      "\n",
      "    star3 star2 star1                 dimensions       weight language  \\\n",
      "0      6%     0     0     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
      "1      9%    4%    6%     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
      "2       0     0     0  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
      "3      5%    2%    2%     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4     10%   10%   10%  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "..    ...   ...   ...                        ...          ...      ...   \n",
      "825    7%    2%    4%       7 x 0.47 x 10 inches  15.5 ounces  English   \n",
      "826    7%    4%    4%  6.14 x 1.25 x 9.21 inches  2.25 pounds  English   \n",
      "827     0     0     0   7.5 x 0.65 x 9.25 inches   1.1 pounds  English   \n",
      "828     0     0     0                          0            0        0   \n",
      "829    4%    1%    1%                          0            0        0   \n",
      "\n",
      "                                             publisher         ISBN_13  \\\n",
      "0       Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1        O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2    Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3      O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4          For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "..                                                 ...             ...   \n",
      "825          Corwin; First edition (December 15, 2017)              59   \n",
      "826      Springer; 1st ed. 2020 edition (July 2, 2020)                   \n",
      "827                    Packt Publishing (July 8, 2022)  978-1803241333   \n",
      "828                                                  0               0   \n",
      "829                                                  0                   \n",
      "\n",
      "                                                  link  \\\n",
      "0    /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1    /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2    /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3    /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4    /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "..                                                 ...   \n",
      "825  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "826  /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "827  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "828  /Clinical-Biostatistics-Epidemiology-Ridiculou...   \n",
      "829  /Machine-Learning-Coders-Programmers-Intellige...   \n",
      "\n",
      "                                         complete_link  \n",
      "0    https://www.amazon.com/Data-Analysis-Using-Low...  \n",
      "1    https://www.amazon.com/Head-First-Data-Analysi...  \n",
      "2    https://www.amazon.com/Guerrilla-Analysis-Usin...  \n",
      "3    https://www.amazon.com/Python-Data-Analysis-Wr...  \n",
      "4    https://www.amazon.com/Excel-Data-Analysis-Dum...  \n",
      "..                                                 ...  \n",
      "825  https://www.amazon.com/Deep-Learning-Engage-Wo...  \n",
      "826  https://www.amazon.com/Machine-Learning-Financ...  \n",
      "827  https://www.amazon.com/gp/slredirect/picassoRe...  \n",
      "828  https://www.amazon.com/Clinical-Biostatistics-...  \n",
      "829  https://www.amazon.com/Machine-Learning-Coders...  \n",
      "\n",
      "[830 rows x 19 columns]\n"
     ]
    }
   ],
   "source": [
    "df_filled_value = df.fillna(0)\n",
    "print(\"\\nDataFrame after filling missing values with 0:\")\n",
    "print(df_filled_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c56bc-6cb6-4fe7-9028-fb6b07b6a7d7",
   "metadata": {},
   "source": [
    "### 5. Fill Missing Values with a Specific Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19394889-1796-4de6-94ac-3af122189cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after forward fill:\n",
      "                                                 title  \\\n",
      "0    Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1    Head First Data Analysis: A learner's guide to...   \n",
      "2    Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3    Python for Data Analysis: Data Wrangling with ...   \n",
      "4    Excel Data Analysis For Dummies (For Dummies (...   \n",
      "..                                                 ...   \n",
      "825   Deep Learning: Engage the World Change the World   \n",
      "826  Machine Learning in Finance: From Theory to Pr...   \n",
      "827  Practical Deep Learning at Scale with MLflow: ...   \n",
      "828  Clinical Biostatistics and Epidemiology Made R...   \n",
      "829  AI and Machine Learning for Coders: A Programm...   \n",
      "\n",
      "                                         author  price  \\\n",
      "0                           [ Dr Dhaval Maheta]   6.75   \n",
      "1                           [ Dr Dhaval Maheta]  33.72   \n",
      "2            [ Oz du Soleil,  and , Bill Jelen]  32.07   \n",
      "3                           [ William McKinney]  53.99   \n",
      "4                             [ Paul McFedries]  24.49   \n",
      "..                                          ...    ...   \n",
      "825     [ Michael Fullan, Joanne Quinn, et al.]   5.33   \n",
      "826  [ Matthew F. Dixon, Igor Halperin, et al.]  55.18   \n",
      "827       [ Yong Liu,  and , Dr. Matei Zaharia]  44.99   \n",
      "828       [ Yong Liu,  and , Dr. Matei Zaharia]  44.99   \n",
      "829                         [ Laurence Moroney]  38.49   \n",
      "\n",
      "    price (including used books) pages  avg_reviews n_reviews star5 star4  \\\n",
      "0                           6.75   500          4.4        23   55%   39%   \n",
      "1                 21.49 - 33.72    484          4.3       124   61%   20%   \n",
      "2                          32.07   274          4.7        10   87%   13%   \n",
      "3                          53.99   547          4.6     1,686   75%   16%   \n",
      "4                          24.49   368          3.9        12   52%   17%   \n",
      "..                           ...   ...          ...       ...   ...   ...   \n",
      "825                8.55 - 35.33    208          4.5        74   72%   14%   \n",
      "826               52.41 - 55.18    573          4.5        93   78%    7%   \n",
      "827                        44.99   288          4.8         8   83%   17%   \n",
      "828                        44.99   288          4.8         8   83%   17%   \n",
      "829                        38.49   288          4.7       142   79%   15%   \n",
      "\n",
      "    star3 star2 star1                 dimensions       weight language  \\\n",
      "0      6%   NaN   NaN     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
      "1      9%    4%    6%     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
      "2      9%    4%    6%  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
      "3      5%    2%    2%     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4     10%   10%   10%  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "..    ...   ...   ...                        ...          ...      ...   \n",
      "825    7%    2%    4%       7 x 0.47 x 10 inches  15.5 ounces  English   \n",
      "826    7%    4%    4%  6.14 x 1.25 x 9.21 inches  2.25 pounds  English   \n",
      "827    7%    4%    4%   7.5 x 0.65 x 9.25 inches   1.1 pounds  English   \n",
      "828    7%    4%    4%   7.5 x 0.65 x 9.25 inches   1.1 pounds  English   \n",
      "829    4%    1%    1%   7.5 x 0.65 x 9.25 inches   1.1 pounds  English   \n",
      "\n",
      "                                             publisher         ISBN_13  \\\n",
      "0       Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1        O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2    Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3      O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4          For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "..                                                 ...             ...   \n",
      "825          Corwin; First edition (December 15, 2017)              59   \n",
      "826      Springer; 1st ed. 2020 edition (July 2, 2020)                   \n",
      "827                    Packt Publishing (July 8, 2022)  978-1803241333   \n",
      "828                    Packt Publishing (July 8, 2022)  978-1803241333   \n",
      "829                    Packt Publishing (July 8, 2022)                   \n",
      "\n",
      "                                                  link  \\\n",
      "0    /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1    /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2    /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3    /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4    /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "..                                                 ...   \n",
      "825  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "826  /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "827  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "828  /Clinical-Biostatistics-Epidemiology-Ridiculou...   \n",
      "829  /Machine-Learning-Coders-Programmers-Intellige...   \n",
      "\n",
      "                                         complete_link  \n",
      "0    https://www.amazon.com/Data-Analysis-Using-Low...  \n",
      "1    https://www.amazon.com/Head-First-Data-Analysi...  \n",
      "2    https://www.amazon.com/Guerrilla-Analysis-Usin...  \n",
      "3    https://www.amazon.com/Python-Data-Analysis-Wr...  \n",
      "4    https://www.amazon.com/Excel-Data-Analysis-Dum...  \n",
      "..                                                 ...  \n",
      "825  https://www.amazon.com/Deep-Learning-Engage-Wo...  \n",
      "826  https://www.amazon.com/Machine-Learning-Financ...  \n",
      "827  https://www.amazon.com/gp/slredirect/picassoRe...  \n",
      "828  https://www.amazon.com/Clinical-Biostatistics-...  \n",
      "829  https://www.amazon.com/Machine-Learning-Coders...  \n",
      "\n",
      "[830 rows x 19 columns]\n",
      "\n",
      "DataFrame after backward fill:\n",
      "                                                 title  \\\n",
      "0    Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1    Head First Data Analysis: A learner's guide to...   \n",
      "2    Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3    Python for Data Analysis: Data Wrangling with ...   \n",
      "4    Excel Data Analysis For Dummies (For Dummies (...   \n",
      "..                                                 ...   \n",
      "825   Deep Learning: Engage the World Change the World   \n",
      "826  Machine Learning in Finance: From Theory to Pr...   \n",
      "827  Practical Deep Learning at Scale with MLflow: ...   \n",
      "828  Clinical Biostatistics and Epidemiology Made R...   \n",
      "829  AI and Machine Learning for Coders: A Programm...   \n",
      "\n",
      "                                         author  price  \\\n",
      "0                           [ Dr Dhaval Maheta]   6.75   \n",
      "1            [ Oz du Soleil,  and , Bill Jelen]  33.72   \n",
      "2            [ Oz du Soleil,  and , Bill Jelen]  32.07   \n",
      "3                           [ William McKinney]  53.99   \n",
      "4                             [ Paul McFedries]  24.49   \n",
      "..                                          ...    ...   \n",
      "825     [ Michael Fullan, Joanne Quinn, et al.]   5.33   \n",
      "826  [ Matthew F. Dixon, Igor Halperin, et al.]  55.18   \n",
      "827       [ Yong Liu,  and , Dr. Matei Zaharia]  44.99   \n",
      "828                         [ Laurence Moroney]  38.49   \n",
      "829                         [ Laurence Moroney]  38.49   \n",
      "\n",
      "    price (including used books) pages  avg_reviews n_reviews star5 star4  \\\n",
      "0                           6.75   500          4.4        23   55%   39%   \n",
      "1                 21.49 - 33.72    484          4.3       124   61%   20%   \n",
      "2                          32.07   274          4.7        10   87%   13%   \n",
      "3                          53.99   547          4.6     1,686   75%   16%   \n",
      "4                          24.49   368          3.9        12   52%   17%   \n",
      "..                           ...   ...          ...       ...   ...   ...   \n",
      "825                8.55 - 35.33    208          4.5        74   72%   14%   \n",
      "826               52.41 - 55.18    573          4.5        93   78%    7%   \n",
      "827                        44.99   288          4.8         8   83%   17%   \n",
      "828                        38.49   NaN          4.7       142   79%   15%   \n",
      "829                        38.49   NaN          4.7       142   79%   15%   \n",
      "\n",
      "    star3 star2 star1                 dimensions       weight language  \\\n",
      "0      6%    4%    6%     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
      "1      9%    4%    6%     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
      "2      5%    2%    2%  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
      "3      5%    2%    2%     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4     10%   10%   10%  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "..    ...   ...   ...                        ...          ...      ...   \n",
      "825    7%    2%    4%       7 x 0.47 x 10 inches  15.5 ounces  English   \n",
      "826    7%    4%    4%  6.14 x 1.25 x 9.21 inches  2.25 pounds  English   \n",
      "827    4%    1%    1%   7.5 x 0.65 x 9.25 inches   1.1 pounds  English   \n",
      "828    4%    1%    1%                        NaN          NaN      NaN   \n",
      "829    4%    1%    1%                        NaN          NaN      NaN   \n",
      "\n",
      "                                             publisher         ISBN_13  \\\n",
      "0       Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1        O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2    Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3      O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4          For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "..                                                 ...             ...   \n",
      "825          Corwin; First edition (December 15, 2017)              59   \n",
      "826      Springer; 1st ed. 2020 edition (July 2, 2020)                   \n",
      "827                    Packt Publishing (July 8, 2022)  978-1803241333   \n",
      "828                                                NaN                   \n",
      "829                                                NaN                   \n",
      "\n",
      "                                                  link  \\\n",
      "0    /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1    /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2    /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3    /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4    /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "..                                                 ...   \n",
      "825  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "826  /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "827  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "828  /Clinical-Biostatistics-Epidemiology-Ridiculou...   \n",
      "829  /Machine-Learning-Coders-Programmers-Intellige...   \n",
      "\n",
      "                                         complete_link  \n",
      "0    https://www.amazon.com/Data-Analysis-Using-Low...  \n",
      "1    https://www.amazon.com/Head-First-Data-Analysi...  \n",
      "2    https://www.amazon.com/Guerrilla-Analysis-Usin...  \n",
      "3    https://www.amazon.com/Python-Data-Analysis-Wr...  \n",
      "4    https://www.amazon.com/Excel-Data-Analysis-Dum...  \n",
      "..                                                 ...  \n",
      "825  https://www.amazon.com/Deep-Learning-Engage-Wo...  \n",
      "826  https://www.amazon.com/Machine-Learning-Financ...  \n",
      "827  https://www.amazon.com/gp/slredirect/picassoRe...  \n",
      "828  https://www.amazon.com/Clinical-Biostatistics-...  \n",
      "829  https://www.amazon.com/Machine-Learning-Coders...  \n",
      "\n",
      "[830 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Ahtasham\\AppData\\Local\\Temp\\ipykernel_20368\\906008488.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_ffill = df.fillna(method='ffill')\n",
      "C:\\Users\\Muhammad Ahtasham\\AppData\\Local\\Temp\\ipykernel_20368\\906008488.py:7: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_bfill = df.fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Forward fill\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(\"\\nDataFrame after forward fill:\")\n",
    "print(df_ffill)\n",
    "\n",
    "# Backward fill\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "print(\"\\nDataFrame after backward fill:\")\n",
    "print(df_bfill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0678d1-0de2-4bf5-be99-714de046fbe5",
   "metadata": {},
   "source": [
    "### 6. Fill Missing Values Using Forward Fill and Backward Fill Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2180018-ad2f-4e0a-acda-c08c1751dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after interpolation:\n",
      "                                                 title  \\\n",
      "0    Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1    Head First Data Analysis: A learner's guide to...   \n",
      "2    Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3    Python for Data Analysis: Data Wrangling with ...   \n",
      "4    Excel Data Analysis For Dummies (For Dummies (...   \n",
      "..                                                 ...   \n",
      "825   Deep Learning: Engage the World Change the World   \n",
      "826  Machine Learning in Finance: From Theory to Pr...   \n",
      "827  Practical Deep Learning at Scale with MLflow: ...   \n",
      "828  Clinical Biostatistics and Epidemiology Made R...   \n",
      "829  AI and Machine Learning for Coders: A Programm...   \n",
      "\n",
      "                                         author  price  \\\n",
      "0                           [ Dr Dhaval Maheta]   6.75   \n",
      "1                                           NaN  33.72   \n",
      "2            [ Oz du Soleil,  and , Bill Jelen]  32.07   \n",
      "3                           [ William McKinney]  53.99   \n",
      "4                             [ Paul McFedries]  24.49   \n",
      "..                                          ...    ...   \n",
      "825     [ Michael Fullan, Joanne Quinn, et al.]   5.33   \n",
      "826  [ Matthew F. Dixon, Igor Halperin, et al.]  55.18   \n",
      "827       [ Yong Liu,  and , Dr. Matei Zaharia]  44.99   \n",
      "828                                         NaN  41.74   \n",
      "829                         [ Laurence Moroney]  38.49   \n",
      "\n",
      "    price (including used books) pages  avg_reviews n_reviews star5 star4  \\\n",
      "0                           6.75   500         4.40        23   55%   39%   \n",
      "1                 21.49 - 33.72    484         4.30       124   61%   20%   \n",
      "2                          32.07   274         4.70        10   87%   13%   \n",
      "3                          53.99   547         4.60     1,686   75%   16%   \n",
      "4                          24.49   368         3.90        12   52%   17%   \n",
      "..                           ...   ...          ...       ...   ...   ...   \n",
      "825                8.55 - 35.33    208         4.50        74   72%   14%   \n",
      "826               52.41 - 55.18    573         4.50        93   78%    7%   \n",
      "827                        44.99   288         4.80         8   83%   17%   \n",
      "828                          NaN   NaN         4.75       NaN   NaN   NaN   \n",
      "829                        38.49   NaN         4.70       142   79%   15%   \n",
      "\n",
      "    star3 star2 star1                 dimensions       weight language  \\\n",
      "0      6%   NaN   NaN     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
      "1      9%    4%    6%     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
      "2     NaN   NaN   NaN  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
      "3      5%    2%    2%     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4     10%   10%   10%  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "..    ...   ...   ...                        ...          ...      ...   \n",
      "825    7%    2%    4%       7 x 0.47 x 10 inches  15.5 ounces  English   \n",
      "826    7%    4%    4%  6.14 x 1.25 x 9.21 inches  2.25 pounds  English   \n",
      "827   NaN   NaN   NaN   7.5 x 0.65 x 9.25 inches   1.1 pounds  English   \n",
      "828   NaN   NaN   NaN                        NaN          NaN      NaN   \n",
      "829    4%    1%    1%                        NaN          NaN      NaN   \n",
      "\n",
      "                                             publisher         ISBN_13  \\\n",
      "0       Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1        O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2    Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3      O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4          For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "..                                                 ...             ...   \n",
      "825          Corwin; First edition (December 15, 2017)              59   \n",
      "826      Springer; 1st ed. 2020 edition (July 2, 2020)                   \n",
      "827                    Packt Publishing (July 8, 2022)  978-1803241333   \n",
      "828                                                NaN             NaN   \n",
      "829                                                NaN                   \n",
      "\n",
      "                                                  link  \\\n",
      "0    /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1    /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2    /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3    /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4    /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "..                                                 ...   \n",
      "825  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "826  /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "827  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "828  /Clinical-Biostatistics-Epidemiology-Ridiculou...   \n",
      "829  /Machine-Learning-Coders-Programmers-Intellige...   \n",
      "\n",
      "                                         complete_link  \n",
      "0    https://www.amazon.com/Data-Analysis-Using-Low...  \n",
      "1    https://www.amazon.com/Head-First-Data-Analysi...  \n",
      "2    https://www.amazon.com/Guerrilla-Analysis-Usin...  \n",
      "3    https://www.amazon.com/Python-Data-Analysis-Wr...  \n",
      "4    https://www.amazon.com/Excel-Data-Analysis-Dum...  \n",
      "..                                                 ...  \n",
      "825  https://www.amazon.com/Deep-Learning-Engage-Wo...  \n",
      "826  https://www.amazon.com/Machine-Learning-Financ...  \n",
      "827  https://www.amazon.com/gp/slredirect/picassoRe...  \n",
      "828  https://www.amazon.com/Clinical-Biostatistics-...  \n",
      "829  https://www.amazon.com/Machine-Learning-Coders...  \n",
      "\n",
      "[830 rows x 19 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Ahtasham\\AppData\\Local\\Temp\\ipykernel_20368\\1632994151.py:1: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df_interpolated = df.interpolate()\n"
     ]
    }
   ],
   "source": [
    "df_interpolated = df.interpolate()\n",
    "print(\"\\nDataFrame after interpolation:\")\n",
    "print(df_interpolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1be9c3-42ec-4ba8-9e63-746c5da1e7b4",
   "metadata": {},
   "source": [
    "### 7. Interpolate Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69fea9f7-0f58-42d3-87cd-356ce2698e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after converting 'price' to integer:\n",
      "title                            object\n",
      "author                           object\n",
      "price                             int32\n",
      "price (including used books)     object\n",
      "pages                            object\n",
      "avg_reviews                     float64\n",
      "n_reviews                        object\n",
      "star5                            object\n",
      "star4                            object\n",
      "star3                            object\n",
      "star2                            object\n",
      "star1                            object\n",
      "dimensions                       object\n",
      "weight                           object\n",
      "language                         object\n",
      "publisher                        object\n",
      "ISBN_13                          object\n",
      "link                             object\n",
      "complete_link                    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "df['price'].fillna(0, inplace=True)  # Replace NaN with 0 (adjust as needed)\n",
    "df['price'] = df['price'].astype(int)\n",
    "print(\"\\nDataFrame after converting 'price' to integer:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d9972d-9b5e-4d97-8cdd-e6ce8c13a71b",
   "metadata": {},
   "source": [
    "### 8. Convert a Column to a Different Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fd7735f-771c-4a46-94d0-fcaea07b3ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after applying a function to 'price':\n",
      "                                               title  \\\n",
      "0  Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1  Head First Data Analysis: A learner's guide to...   \n",
      "2  Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3  Python for Data Analysis: Data Wrangling with ...   \n",
      "4  Excel Data Analysis For Dummies (For Dummies (...   \n",
      "\n",
      "                               author  price price (including used books)  \\\n",
      "0                 [ Dr Dhaval Maheta]    6.6                         6.75   \n",
      "1                                 NaN   36.3               21.49 - 33.72    \n",
      "2  [ Oz du Soleil,  and , Bill Jelen]   35.2                        32.07   \n",
      "3                 [ William McKinney]   58.3                        53.99   \n",
      "4                   [ Paul McFedries]   26.4                        24.49   \n",
      "\n",
      "  pages  avg_reviews n_reviews star5 star4 star3 star2 star1  \\\n",
      "0   500          4.4        23   55%   39%    6%   NaN   NaN   \n",
      "1   484          4.3       124   61%   20%    9%    4%    6%   \n",
      "2   274          4.7        10   87%   13%   NaN   NaN   NaN   \n",
      "3   547          4.6     1,686   75%   16%    5%    2%    2%   \n",
      "4   368          3.9        12   52%   17%   10%   10%   10%   \n",
      "\n",
      "                  dimensions       weight language  \\\n",
      "0     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
      "1     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
      "2  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
      "3     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "\n",
      "                                           publisher         ISBN_13  \\\n",
      "0     Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1      O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2  Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3    O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4        For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "\n",
      "                                                link  \\\n",
      "0  /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1  /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2  /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3  /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4  /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "\n",
      "                                       complete_link  \n",
      "0  https://www.amazon.com/Data-Analysis-Using-Low...  \n",
      "1  https://www.amazon.com/Head-First-Data-Analysi...  \n",
      "2  https://www.amazon.com/Guerrilla-Analysis-Usin...  \n",
      "3  https://www.amazon.com/Python-Data-Analysis-Wr...  \n",
      "4  https://www.amazon.com/Excel-Data-Analysis-Dum...  \n"
     ]
    }
   ],
   "source": [
    "df['price'] = df['price'].apply(lambda x: x * 1.1)\n",
    "print(\"\\nDataFrame after applying a function to 'price':\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7162a342-3edd-4d61-84a5-2efda81f122e",
   "metadata": {},
   "source": [
    "### 9. Apply a Function to Transform the Values of a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b9d8567-cdb6-40d2-bd3b-d7e1ee5e434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after Min-Max scaling of 'price':\n",
      "   price  price_normalized\n",
      "0    6.6          0.004552\n",
      "1   36.3          0.025038\n",
      "2   35.2          0.024279\n",
      "3   58.3          0.040212\n",
      "4   26.4          0.018209\n"
     ]
    }
   ],
   "source": [
    "df['price_normalized'] = (df['price'] - df['price'].min()) / (df['price'].max() - df['price'].min())\n",
    "print(\"\\nDataFrame after Min-Max scaling of 'price':\")\n",
    "print(df[['price', 'price_normalized']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222dfb25-569d-4f8c-bf24-1f34410e5fab",
   "metadata": {},
   "source": [
    "### 10. Normalize a Column Using Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa2194f9-e07b-40ab-acde-d50b776b7ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after Min-Max scaling of 'price':\n",
      "   price  price_normalized\n",
      "0    6.6          0.004552\n",
      "1   36.3          0.025038\n",
      "2   35.2          0.024279\n",
      "3   58.3          0.040212\n",
      "4   26.4          0.018209\n"
     ]
    }
   ],
   "source": [
    "df['price_normalized'] = (df['price'] - df['price'].min()) / (df['price'].max() - df['price'].min())\n",
    "print(\"\\nDataFrame after Min-Max scaling of 'price':\")\n",
    "print(df[['price', 'price_normalized']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51a39a-f5cd-4059-83f6-809de5af9c42",
   "metadata": {},
   "source": [
    "### 11. Standardize a Column (Z-Score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82e3ae13-9bcc-4abc-89e0-426c25b69fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after z-score normalization of 'price':\n",
      "   price  price_standardized\n",
      "0    6.6           -0.554591\n",
      "1   36.3           -0.112237\n",
      "2   35.2           -0.128620\n",
      "3   58.3            0.215433\n",
      "4   26.4           -0.259688\n"
     ]
    }
   ],
   "source": [
    "df['price_standardized'] = (df['price'] - df['price'].mean()) / df['price'].std()\n",
    "print(\"\\nDataFrame after z-score normalization of 'price':\")\n",
    "print(df[['price', 'price_standardized']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2922704d-dd5a-4e7c-ba99-0574fa440f99",
   "metadata": {},
   "source": [
    "### 12. Identify Duplicate Rows in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6074fc3c-deb3-4dea-9a91-9050f738e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate rows in the DataFrame:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = df.duplicated()\n",
    "print(\"\\nDuplicate rows in the DataFrame:\")\n",
    "print(duplicate_rows.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a61cb7-5e9b-4859-829e-c0be87fc4b06",
   "metadata": {},
   "source": [
    "### 13. Drop Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2318067-a572-4328-a5d8-d2dd5dc81557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping duplicate rows:\n",
      "                                                 title  \\\n",
      "0    Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1    Head First Data Analysis: A learner's guide to...   \n",
      "2    Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3    Python for Data Analysis: Data Wrangling with ...   \n",
      "4    Excel Data Analysis For Dummies (For Dummies (...   \n",
      "..                                                 ...   \n",
      "825   Deep Learning: Engage the World Change the World   \n",
      "826  Machine Learning in Finance: From Theory to Pr...   \n",
      "827  Practical Deep Learning at Scale with MLflow: ...   \n",
      "828  Clinical Biostatistics and Epidemiology Made R...   \n",
      "829  AI and Machine Learning for Coders: A Programm...   \n",
      "\n",
      "                                         author  price  \\\n",
      "0                           [ Dr Dhaval Maheta]    6.6   \n",
      "1                                           NaN   36.3   \n",
      "2            [ Oz du Soleil,  and , Bill Jelen]   35.2   \n",
      "3                           [ William McKinney]   58.3   \n",
      "4                             [ Paul McFedries]   26.4   \n",
      "..                                          ...    ...   \n",
      "825     [ Michael Fullan, Joanne Quinn, et al.]    5.5   \n",
      "826  [ Matthew F. Dixon, Igor Halperin, et al.]   60.5   \n",
      "827       [ Yong Liu,  and , Dr. Matei Zaharia]   48.4   \n",
      "828                                         NaN    0.0   \n",
      "829                         [ Laurence Moroney]   41.8   \n",
      "\n",
      "    price (including used books) pages  avg_reviews n_reviews star5 star4  \\\n",
      "0                           6.75   500          4.4        23   55%   39%   \n",
      "1                 21.49 - 33.72    484          4.3       124   61%   20%   \n",
      "2                          32.07   274          4.7        10   87%   13%   \n",
      "3                          53.99   547          4.6     1,686   75%   16%   \n",
      "4                          24.49   368          3.9        12   52%   17%   \n",
      "..                           ...   ...          ...       ...   ...   ...   \n",
      "825                8.55 - 35.33    208          4.5        74   72%   14%   \n",
      "826               52.41 - 55.18    573          4.5        93   78%    7%   \n",
      "827                        44.99   288          4.8         8   83%   17%   \n",
      "828                          NaN   NaN          NaN       NaN   NaN   NaN   \n",
      "829                        38.49   NaN          4.7       142   79%   15%   \n",
      "\n",
      "    star3  ... star1                 dimensions       weight language  \\\n",
      "0      6%  ...   NaN     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
      "1      9%  ...    6%     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
      "2     NaN  ...   NaN  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
      "3      5%  ...    2%     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4     10%  ...   10%  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "..    ...  ...   ...                        ...          ...      ...   \n",
      "825    7%  ...    4%       7 x 0.47 x 10 inches  15.5 ounces  English   \n",
      "826    7%  ...    4%  6.14 x 1.25 x 9.21 inches  2.25 pounds  English   \n",
      "827   NaN  ...   NaN   7.5 x 0.65 x 9.25 inches   1.1 pounds  English   \n",
      "828   NaN  ...   NaN                        NaN          NaN      NaN   \n",
      "829    4%  ...    1%                        NaN          NaN      NaN   \n",
      "\n",
      "                                             publisher         ISBN_13  \\\n",
      "0       Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1        O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2    Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3      O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4          For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "..                                                 ...             ...   \n",
      "825          Corwin; First edition (December 15, 2017)              59   \n",
      "826      Springer; 1st ed. 2020 edition (July 2, 2020)                   \n",
      "827                    Packt Publishing (July 8, 2022)  978-1803241333   \n",
      "828                                                NaN             NaN   \n",
      "829                                                NaN                   \n",
      "\n",
      "                                                  link  \\\n",
      "0    /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1    /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2    /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3    /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4    /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "..                                                 ...   \n",
      "825  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "826  /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "827  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "828  /Clinical-Biostatistics-Epidemiology-Ridiculou...   \n",
      "829  /Machine-Learning-Coders-Programmers-Intellige...   \n",
      "\n",
      "                                         complete_link price_normalized  \\\n",
      "0    https://www.amazon.com/Data-Analysis-Using-Low...         0.004552   \n",
      "1    https://www.amazon.com/Head-First-Data-Analysi...         0.025038   \n",
      "2    https://www.amazon.com/Guerrilla-Analysis-Usin...         0.024279   \n",
      "3    https://www.amazon.com/Python-Data-Analysis-Wr...         0.040212   \n",
      "4    https://www.amazon.com/Excel-Data-Analysis-Dum...         0.018209   \n",
      "..                                                 ...              ...   \n",
      "825  https://www.amazon.com/Deep-Learning-Engage-Wo...         0.003794   \n",
      "826  https://www.amazon.com/Machine-Learning-Financ...         0.041730   \n",
      "827  https://www.amazon.com/gp/slredirect/picassoRe...         0.033384   \n",
      "828  https://www.amazon.com/Clinical-Biostatistics-...         0.000000   \n",
      "829  https://www.amazon.com/Machine-Learning-Coders...         0.028832   \n",
      "\n",
      "     price_standardized  \n",
      "0             -0.554591  \n",
      "1             -0.112237  \n",
      "2             -0.128620  \n",
      "3              0.215433  \n",
      "4             -0.259688  \n",
      "..                  ...  \n",
      "825           -0.570975  \n",
      "826            0.248200  \n",
      "827            0.067982  \n",
      "828           -0.652892  \n",
      "829           -0.030319  \n",
      "\n",
      "[830 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(\"\\nDataFrame after dropping duplicate rows:\")\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c9c45-1297-4f24-806b-783a63a68b1d",
   "metadata": {},
   "source": [
    "### 14. Drop Duplicate Rows Based on Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f5c8b4a-8df0-494b-aba9-4529261638a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after dropping duplicate rows based on 'title' and 'author':\n",
      "                                                 title  \\\n",
      "0    Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1    Head First Data Analysis: A learner's guide to...   \n",
      "2    Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3    Python for Data Analysis: Data Wrangling with ...   \n",
      "4    Excel Data Analysis For Dummies (For Dummies (...   \n",
      "..                                                 ...   \n",
      "825   Deep Learning: Engage the World Change the World   \n",
      "826  Machine Learning in Finance: From Theory to Pr...   \n",
      "827  Practical Deep Learning at Scale with MLflow: ...   \n",
      "828  Clinical Biostatistics and Epidemiology Made R...   \n",
      "829  AI and Machine Learning for Coders: A Programm...   \n",
      "\n",
      "                                         author  price  \\\n",
      "0                           [ Dr Dhaval Maheta]    6.6   \n",
      "1                                           NaN   36.3   \n",
      "2            [ Oz du Soleil,  and , Bill Jelen]   35.2   \n",
      "3                           [ William McKinney]   58.3   \n",
      "4                             [ Paul McFedries]   26.4   \n",
      "..                                          ...    ...   \n",
      "825     [ Michael Fullan, Joanne Quinn, et al.]    5.5   \n",
      "826  [ Matthew F. Dixon, Igor Halperin, et al.]   60.5   \n",
      "827       [ Yong Liu,  and , Dr. Matei Zaharia]   48.4   \n",
      "828                                         NaN    0.0   \n",
      "829                         [ Laurence Moroney]   41.8   \n",
      "\n",
      "    price (including used books) pages  avg_reviews n_reviews star5 star4  \\\n",
      "0                           6.75   500          4.4        23   55%   39%   \n",
      "1                 21.49 - 33.72    484          4.3       124   61%   20%   \n",
      "2                          32.07   274          4.7        10   87%   13%   \n",
      "3                          53.99   547          4.6     1,686   75%   16%   \n",
      "4                          24.49   368          3.9        12   52%   17%   \n",
      "..                           ...   ...          ...       ...   ...   ...   \n",
      "825                8.55 - 35.33    208          4.5        74   72%   14%   \n",
      "826               52.41 - 55.18    573          4.5        93   78%    7%   \n",
      "827                        44.99   288          4.8         8   83%   17%   \n",
      "828                          NaN   NaN          NaN       NaN   NaN   NaN   \n",
      "829                        38.49   NaN          4.7       142   79%   15%   \n",
      "\n",
      "    star3  ... star1                 dimensions       weight language  \\\n",
      "0      6%  ...   NaN     8.5 x 1.01 x 11 inches  2.53 pounds  English   \n",
      "1      9%  ...    6%     8 x 0.98 x 9.25 inches  1.96 pounds  English   \n",
      "2     NaN  ...   NaN  8.25 x 0.6 x 10.75 inches   1.4 pounds  English   \n",
      "3      5%  ...    2%     7 x 1.11 x 9.19 inches  1.47 pounds  English   \n",
      "4     10%  ...   10%  7.38 x 0.83 x 9.25 inches   1.3 pounds  English   \n",
      "..    ...  ...   ...                        ...          ...      ...   \n",
      "825    7%  ...    4%       7 x 0.47 x 10 inches  15.5 ounces  English   \n",
      "826    7%  ...    4%  6.14 x 1.25 x 9.21 inches  2.25 pounds  English   \n",
      "827   NaN  ...   NaN   7.5 x 0.65 x 9.25 inches   1.1 pounds  English   \n",
      "828   NaN  ...   NaN                        NaN          NaN      NaN   \n",
      "829    4%  ...    1%                        NaN          NaN      NaN   \n",
      "\n",
      "                                             publisher         ISBN_13  \\\n",
      "0       Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1        O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2    Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3      O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4          For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "..                                                 ...             ...   \n",
      "825          Corwin; First edition (December 15, 2017)              59   \n",
      "826      Springer; 1st ed. 2020 edition (July 2, 2020)                   \n",
      "827                    Packt Publishing (July 8, 2022)  978-1803241333   \n",
      "828                                                NaN             NaN   \n",
      "829                                                NaN                   \n",
      "\n",
      "                                                  link  \\\n",
      "0    /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1    /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2    /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3    /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4    /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "..                                                 ...   \n",
      "825  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "826  /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "827  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "828  /Clinical-Biostatistics-Epidemiology-Ridiculou...   \n",
      "829  /Machine-Learning-Coders-Programmers-Intellige...   \n",
      "\n",
      "                                         complete_link price_normalized  \\\n",
      "0    https://www.amazon.com/Data-Analysis-Using-Low...         0.004552   \n",
      "1    https://www.amazon.com/Head-First-Data-Analysi...         0.025038   \n",
      "2    https://www.amazon.com/Guerrilla-Analysis-Usin...         0.024279   \n",
      "3    https://www.amazon.com/Python-Data-Analysis-Wr...         0.040212   \n",
      "4    https://www.amazon.com/Excel-Data-Analysis-Dum...         0.018209   \n",
      "..                                                 ...              ...   \n",
      "825  https://www.amazon.com/Deep-Learning-Engage-Wo...         0.003794   \n",
      "826  https://www.amazon.com/Machine-Learning-Financ...         0.041730   \n",
      "827  https://www.amazon.com/gp/slredirect/picassoRe...         0.033384   \n",
      "828  https://www.amazon.com/Clinical-Biostatistics-...         0.000000   \n",
      "829  https://www.amazon.com/Machine-Learning-Coders...         0.028832   \n",
      "\n",
      "     price_standardized  \n",
      "0             -0.554591  \n",
      "1             -0.112237  \n",
      "2             -0.128620  \n",
      "3              0.215433  \n",
      "4             -0.259688  \n",
      "..                  ...  \n",
      "825           -0.570975  \n",
      "826            0.248200  \n",
      "827            0.067982  \n",
      "828           -0.652892  \n",
      "829           -0.030319  \n",
      "\n",
      "[830 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "df_no_duplicates_specified = df.drop_duplicates(subset=['title', 'author'])\n",
    "print(\"\\nDataFrame after dropping duplicate rows based on 'title' and 'author':\")\n",
    "print(df_no_duplicates_specified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8cd99-cbc7-4c7d-b2e6-73ec0bfb9e49",
   "metadata": {},
   "source": [
    "### 15. Convert All String Values in a Column to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eae65e2e-848c-4c2f-9841-b08e7bbc7a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after converting 'author' to lowercase:\n",
      "                               author\n",
      "0                 [ dr dhaval maheta]\n",
      "1                                 NaN\n",
      "2  [ oz du soleil,  and , bill jelen]\n",
      "3                 [ william mckinney]\n",
      "4                   [ paul mcfedries]\n"
     ]
    }
   ],
   "source": [
    "df['author'] = df['author'].str.lower()\n",
    "print(\"\\nDataFrame after converting 'author' to lowercase:\")\n",
    "print(df[['author']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7ad5a-4f44-4b0c-bbd4-bf03806c0d0d",
   "metadata": {},
   "source": [
    "### 16. Remove Leading and Trailing Spaces from String Values in a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa85913e-c25a-4c0f-ac4b-e82f9dc91a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after removing leading and trailing spaces from 'title':\n",
      "                                               title\n",
      "0  Data Analysis Using R (Low Priced Edition): A ...\n",
      "1  Head First Data Analysis: A learner's guide to...\n",
      "2  Guerrilla Data Analysis Using Microsoft Excel:...\n",
      "3  Python for Data Analysis: Data Wrangling with ...\n",
      "4  Excel Data Analysis For Dummies (For Dummies (...\n"
     ]
    }
   ],
   "source": [
    "df['title'] = df['title'].str.strip()\n",
    "print(\"\\nDataFrame after removing leading and trailing spaces from 'title':\")\n",
    "print(df[['title']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd92dcb-84e9-48e8-84db-357d7c4c8a19",
   "metadata": {},
   "source": [
    "### 17. Replace a Specific Substring in a Column with Another Substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf7d58fb-9d62-4fc9-aa6c-757561130883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after replacing 'book' with 'novel' in 'title':\n",
      "                                               title\n",
      "0  Data Analysis Using R (Low Priced Edition): A ...\n",
      "1  Head First Data Analysis: A learner's guide to...\n",
      "2  Guerrilla Data Analysis Using Microsoft Excel:...\n",
      "3  Python for Data Analysis: Data Wrangling with ...\n",
      "4  Excel Data Analysis For Dummies (For Dummies (...\n"
     ]
    }
   ],
   "source": [
    "df['title'] = df['title'].str.replace('book', 'novel')\n",
    "print(\"\\nDataFrame after replacing 'book' with 'novel' in 'title':\")\n",
    "print(df[['title']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36876f63-8a52-47c0-83e2-de9b18dae7ea",
   "metadata": {},
   "source": [
    "### 18. Extract a Substring from Each Value in a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e39c608-868e-4336-9733-f7c57d069689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after extracting first 10 characters of 'title':\n",
      "                                               title title_short\n",
      "0  Data Analysis Using R (Low Priced Edition): A ...  Data Analy\n",
      "1  Head First Data Analysis: A learner's guide to...  Head First\n",
      "2  Guerrilla Data Analysis Using Microsoft Excel:...  Guerrilla \n",
      "3  Python for Data Analysis: Data Wrangling with ...  Python for\n",
      "4  Excel Data Analysis For Dummies (For Dummies (...  Excel Data\n"
     ]
    }
   ],
   "source": [
    "df['title_short'] = df['title'].str[:10]\n",
    "print(\"\\nDataFrame after extracting first 10 characters of 'title':\")\n",
    "print(df[['title', 'title_short']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d334d6-81e0-4621-b553-7ffaf645f4ef",
   "metadata": {},
   "source": [
    "### 19. Convert a Categorical Column to Numerical Using One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14e17136-1af3-4ef0-a1c2-bef84793d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after one-hot encoding 'language':\n",
      "                                               title  \\\n",
      "0  Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1  Head First Data Analysis: A learner's guide to...   \n",
      "2  Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3  Python for Data Analysis: Data Wrangling with ...   \n",
      "4  Excel Data Analysis For Dummies (For Dummies (...   \n",
      "\n",
      "                               author  price price (including used books)  \\\n",
      "0                 [ dr dhaval maheta]    6.6                         6.75   \n",
      "1                                 NaN   36.3               21.49 - 33.72    \n",
      "2  [ oz du soleil,  and , bill jelen]   35.2                        32.07   \n",
      "3                 [ william mckinney]   58.3                        53.99   \n",
      "4                   [ paul mcfedries]   26.4                        24.49   \n",
      "\n",
      "  pages  avg_reviews n_reviews star5 star4 star3  ... language_English  \\\n",
      "0   500          4.4        23   55%   39%    6%  ...             True   \n",
      "1   484          4.3       124   61%   20%    9%  ...             True   \n",
      "2   274          4.7        10   87%   13%   NaN  ...             True   \n",
      "3   547          4.6     1,686   75%   16%    5%  ...             True   \n",
      "4   368          3.9        12   52%   17%   10%  ...             True   \n",
      "\n",
      "  language_English (DTS-HD Master Audio 5.1), French (DTS-HD 2.0)  \\\n",
      "0                                              False                \n",
      "1                                              False                \n",
      "2                                              False                \n",
      "3                                              False                \n",
      "4                                              False                \n",
      "\n",
      "  language_English (Dolby Digital 2.0 Mono)  \\\n",
      "0                                     False   \n",
      "1                                     False   \n",
      "2                                     False   \n",
      "3                                     False   \n",
      "4                                     False   \n",
      "\n",
      "  language_Scroll to the top of the page and click the  language_Spanish  \\\n",
      "0                                              False               False   \n",
      "1                                              False               False   \n",
      "2                                              False               False   \n",
      "3                                              False               False   \n",
      "4                                              False               False   \n",
      "\n",
      "  language_This Python programming guide assumes certain level of programming knowledge. It is not a beginner textbook.  \\\n",
      "0                                              False                                                                      \n",
      "1                                              False                                                                      \n",
      "2                                              False                                                                      \n",
      "3                                              False                                                                      \n",
      "4                                              False                                                                      \n",
      "\n",
      "  language_Unqualified, Japanese (Dolby Digital 2.0 Mono), English (Dolby Digital 5.1), English (Dolby Digital 2.0 Mono)  \\\n",
      "0                                              False                                                                       \n",
      "1                                              False                                                                       \n",
      "2                                              False                                                                       \n",
      "3                                              False                                                                       \n",
      "4                                              False                                                                       \n",
      "\n",
      "  language_standard library  language_you will discover all you need   \\\n",
      "0                     False                                     False   \n",
      "1                     False                                     False   \n",
      "2                     False                                     False   \n",
      "3                     False                                     False   \n",
      "4                     False                                     False   \n",
      "\n",
      "   language_• How to make better business decisions using   \n",
      "0                                              False        \n",
      "1                                              False        \n",
      "2                                              False        \n",
      "3                                              False        \n",
      "4                                              False        \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "df_one_hot_encoded = pd.get_dummies(df, columns=['language'])\n",
    "print(\"\\nDataFrame after one-hot encoding 'language':\")\n",
    "print(df_one_hot_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0992892-2680-4a1c-9a87-93e4d8d4f5eb",
   "metadata": {},
   "source": [
    "### 20. Convert a Categorical Column to Numerical Using Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45391fff-8a4d-4fc0-bf71-bac474ae3c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after label encoding 'publisher':\n",
      "                                           publisher  publisher_encoded\n",
      "0     Notion Press Media Pvt Ltd (November 22, 2021)                344\n",
      "1      O'Reilly Media; 1st edition (August 18, 2009)                350\n",
      "2  Holy Macro! Books; Third edition (August 1, 2022)                160\n",
      "3    O'Reilly Media; 2nd edition (November 14, 2017)                408\n",
      "4        For Dummies; 5th edition (February 3, 2022)                150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['publisher_encoded'] = label_encoder.fit_transform(df['publisher'])\n",
    "print(\"\\nDataFrame after label encoding 'publisher':\")\n",
    "print(df[['publisher', 'publisher_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd4079-dbe1-4ab5-8493-3f8398beb3e8",
   "metadata": {},
   "source": [
    "### 21. Group Values in a Categorical Column and Create a New Column with Grouped Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03108d48-4dc7-42bf-9e41-f0b9256049f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           publisher publisher_grouped\n",
      "0     Notion Press Media Pvt Ltd (November 22, 2021)             Other\n",
      "1      O'Reilly Media; 1st edition (August 18, 2009)             Other\n",
      "2  Holy Macro! Books; Third edition (August 1, 2022)             Other\n",
      "3    O'Reilly Media; 2nd edition (November 14, 2017)             Other\n",
      "4        For Dummies; 5th edition (February 3, 2022)             Other\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['publisher'], inplace=True)  # Drops rows with NaN in 'publisher'\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "df['publisher_grouped'] = df['publisher'].apply(lambda x: 'Other' if publisher_counts[x] < 5 else x)\n",
    "print(df[['publisher', 'publisher_grouped']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486aa767-4969-4002-aab5-b7f12c028e82",
   "metadata": {},
   "source": [
    "### 22. Merge Two DataFrames Based on a Common Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32f465fc-32e9-4796-98b9-e273ccad033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after merging with additional data:\n",
      "                                               title  \\\n",
      "0  Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1  Head First Data Analysis: A learner's guide to...   \n",
      "2  Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3  Python for Data Analysis: Data Wrangling with ...   \n",
      "4  Excel Data Analysis For Dummies (For Dummies (...   \n",
      "\n",
      "                               author  price price (including used books)  \\\n",
      "0                 [ dr dhaval maheta]    6.6                         6.75   \n",
      "1                                 NaN   36.3               21.49 - 33.72    \n",
      "2  [ oz du soleil,  and , bill jelen]   35.2                        32.07   \n",
      "3                 [ william mckinney]   58.3                        53.99   \n",
      "4                   [ paul mcfedries]   26.4                        24.49   \n",
      "\n",
      "  pages  avg_reviews n_reviews star5 star4 star3  ...  \\\n",
      "0   500          4.4        23   55%   39%    6%  ...   \n",
      "1   484          4.3       124   61%   20%    9%  ...   \n",
      "2   274          4.7        10   87%   13%   NaN  ...   \n",
      "3   547          4.6     1,686   75%   16%    5%  ...   \n",
      "4   368          3.9        12   52%   17%   10%  ...   \n",
      "\n",
      "                                           publisher         ISBN_13  \\\n",
      "0     Notion Press Media Pvt Ltd (November 22, 2021)  978-1685549596   \n",
      "1      O'Reilly Media; 1st edition (August 18, 2009)  978-0596153939   \n",
      "2  Holy Macro! Books; Third edition (August 1, 2022)  978-1615470747   \n",
      "3    O'Reilly Media; 2nd edition (November 14, 2017)  978-1491957660   \n",
      "4        For Dummies; 5th edition (February 3, 2022)  978-1119844426   \n",
      "\n",
      "                                                link  \\\n",
      "0  /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1  /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2  /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3  /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4  /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "\n",
      "                                       complete_link price_normalized  \\\n",
      "0  https://www.amazon.com/Data-Analysis-Using-Low...         0.004552   \n",
      "1  https://www.amazon.com/Head-First-Data-Analysi...         0.025038   \n",
      "2  https://www.amazon.com/Guerrilla-Analysis-Usin...         0.024279   \n",
      "3  https://www.amazon.com/Python-Data-Analysis-Wr...         0.040212   \n",
      "4  https://www.amazon.com/Excel-Data-Analysis-Dum...         0.018209   \n",
      "\n",
      "  price_standardized title_short publisher_encoded publisher_grouped  Genre  \n",
      "0          -0.554591  Data Analy               344             Other    NaN  \n",
      "1          -0.112237  Head First               350             Other    NaN  \n",
      "2          -0.128620  Guerrilla                160             Other    NaN  \n",
      "3           0.215433  Python for               408             Other    NaN  \n",
      "4          -0.259688  Excel Data               150             Other    NaN  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "additional_data = {\n",
    "    'ISBN_13': ['978-3-16-148410-0', '978-1-86197-876-9'],\n",
    "    'Genre': ['Fiction', 'Non-Fiction']\n",
    "}\n",
    "df_additional = pd.DataFrame(additional_data)\n",
    "\n",
    "df_merged = pd.merge(df, df_additional, on='ISBN_13', how='left')\n",
    "print(\"\\nDataFrame after merging with additional data:\")\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647f73b-e9f0-4182-99a4-01b345072895",
   "metadata": {},
   "source": [
    "### 23. Concatenate Two DataFrames Vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92278615-3e47-4af8-8d0c-ab171b71ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after vertical concatenation:\n",
      "                                                 title  \\\n",
      "711   Deep Learning: Engage the World Change the World   \n",
      "712  Machine Learning in Finance: From Theory to Pr...   \n",
      "713  Practical Deep Learning at Scale with MLflow: ...   \n",
      "714                                         New Book 1   \n",
      "715                                         New Book 2   \n",
      "\n",
      "                                         author  price  \\\n",
      "711     [ michael fullan, joanne quinn, et al.]    5.5   \n",
      "712  [ matthew f. dixon, igor halperin, et al.]   60.5   \n",
      "713       [ yong liu,  and , dr. matei zaharia]   48.4   \n",
      "714                                    Author A   20.0   \n",
      "715                                    Author B   30.0   \n",
      "\n",
      "    price (including used books) pages  avg_reviews n_reviews star5 star4  \\\n",
      "711                8.55 - 35.33    208          4.5        74   72%   14%   \n",
      "712               52.41 - 55.18    573          4.5        93   78%    7%   \n",
      "713                        44.99   288          4.8         8   83%   17%   \n",
      "714                         15.0   300          4.5       100    50    30   \n",
      "715                         25.0   400          4.0       150    60    50   \n",
      "\n",
      "    star3  ... language                                      publisher  \\\n",
      "711    7%  ...  English      Corwin; First edition (December 15, 2017)   \n",
      "712    7%  ...  English  Springer; 1st ed. 2020 edition (July 2, 2020)   \n",
      "713   NaN  ...  English                Packt Publishing (July 8, 2022)   \n",
      "714    10  ...  English                                    Publisher A   \n",
      "715    20  ...  English                                    Publisher B   \n",
      "\n",
      "               ISBN_13                                               link  \\\n",
      "711                 59  /Deep-Learning-Engage-World-Change/dp/15063685...   \n",
      "712                     /Machine-Learning-Finance-Theory-Practice/dp/3...   \n",
      "713     978-1803241333  /gp/slredirect/picassoRedirect.html/ref=pa_sp_...   \n",
      "714  123-4-567-89012-3                                              link1   \n",
      "715  123-4-567-89012-4                                              link2   \n",
      "\n",
      "                                         complete_link price_normalized  \\\n",
      "711  https://www.amazon.com/Deep-Learning-Engage-Wo...         0.003794   \n",
      "712  https://www.amazon.com/Machine-Learning-Financ...         0.041730   \n",
      "713  https://www.amazon.com/gp/slredirect/picassoRe...         0.033384   \n",
      "714                                     complete_link1              NaN   \n",
      "715                                     complete_link2              NaN   \n",
      "\n",
      "    price_standardized title_short publisher_encoded  publisher_grouped  \n",
      "711          -0.570975  Deep Learn             110.0              Other  \n",
      "712           0.248200  Machine Le             609.0              Other  \n",
      "713           0.067982  Practical              462.0              Other  \n",
      "714                NaN         NaN               NaN                NaN  \n",
      "715                NaN         NaN               NaN                NaN  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "additional_rows = pd.DataFrame({\n",
    "    'title': ['New Book 1', 'New Book 2'],\n",
    "    'author': ['Author A', 'Author B'],\n",
    "    'price': [20.0, 30.0],\n",
    "    'price (including used books)': [15.0, 25.0],\n",
    "    'pages': [300, 400],\n",
    "    'avg_reviews': [4.5, 4.0],\n",
    "    'n_reviews': [100, 150],\n",
    "    'star5': [50, 60],\n",
    "    'star4': [30, 50],\n",
    "    'star3': [10, 20],\n",
    "    'star2': [5, 10],\n",
    "    'star1': [5, 10],\n",
    "    'dimensions': ['8 x 5 x 1', '9 x 6 x 1.5'],\n",
    "    'weight': [1.0, 1.5],\n",
    "    'language': ['English', 'English'],\n",
    "    'publisher': ['Publisher A', 'Publisher B'],\n",
    "    'ISBN_13': ['123-4-567-89012-3', '123-4-567-89012-4'],\n",
    "    'link': ['link1', 'link2'],\n",
    "    'complete_link': ['complete_link1', 'complete_link2']\n",
    "})\n",
    "\n",
    "df_concatenated_vertical = pd.concat([df, additional_rows], axis=0, ignore_index=True)\n",
    "print(\"\\nDataFrame after vertical concatenation:\")\n",
    "print(df_concatenated_vertical.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe2dae-d426-4fe0-a267-11b59245fa01",
   "metadata": {},
   "source": [
    "### 24. Concatenate Two DataFrames Horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f86e55f-12d6-4b46-818c-c88a57ba2566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after horizontal concatenation:\n",
      "                                               title  \\\n",
      "0  Data Analysis Using R (Low Priced Edition): A ...   \n",
      "1  Head First Data Analysis: A learner's guide to...   \n",
      "2  Guerrilla Data Analysis Using Microsoft Excel:...   \n",
      "3  Python for Data Analysis: Data Wrangling with ...   \n",
      "4  Excel Data Analysis For Dummies (For Dummies (...   \n",
      "\n",
      "                               author  price price (including used books)  \\\n",
      "0                 [ dr dhaval maheta]    6.6                         6.75   \n",
      "1                                 NaN   36.3               21.49 - 33.72    \n",
      "2  [ oz du soleil,  and , bill jelen]   35.2                        32.07   \n",
      "3                 [ william mckinney]   58.3                        53.99   \n",
      "4                   [ paul mcfedries]   26.4                        24.49   \n",
      "\n",
      "  pages  avg_reviews n_reviews star5 star4 star3  ...         ISBN_13  \\\n",
      "0   500          4.4        23   55%   39%    6%  ...  978-1685549596   \n",
      "1   484          4.3       124   61%   20%    9%  ...  978-0596153939   \n",
      "2   274          4.7        10   87%   13%   NaN  ...  978-1615470747   \n",
      "3   547          4.6     1,686   75%   16%    5%  ...  978-1491957660   \n",
      "4   368          3.9        12   52%   17%   10%  ...  978-1119844426   \n",
      "\n",
      "                                                link  \\\n",
      "0  /Data-Analysis-Using-Low-Priced/dp/1685549594/...   \n",
      "1  /Head-First-Data-Analysis-statistics/dp/059615...   \n",
      "2  /Guerrilla-Analysis-Using-Microsoft-Excel/dp/1...   \n",
      "3  /Python-Data-Analysis-Wrangling-IPython/dp/149...   \n",
      "4  /Excel-Data-Analysis-Dummies-Computer/dp/11198...   \n",
      "\n",
      "                                       complete_link price_normalized  \\\n",
      "0  https://www.amazon.com/Data-Analysis-Using-Low...         0.004552   \n",
      "1  https://www.amazon.com/Head-First-Data-Analysi...         0.025038   \n",
      "2  https://www.amazon.com/Guerrilla-Analysis-Usin...         0.024279   \n",
      "3  https://www.amazon.com/Python-Data-Analysis-Wr...         0.040212   \n",
      "4  https://www.amazon.com/Excel-Data-Analysis-Dum...         0.018209   \n",
      "\n",
      "  price_standardized title_short publisher_encoded publisher_grouped new_col1  \\\n",
      "0          -0.554591  Data Analy               344             Other   value1   \n",
      "1          -0.112237  Head First               350             Other   value2   \n",
      "2          -0.128620  Guerrilla                160             Other   value3   \n",
      "3           0.215433  Python for               408             Other   value4   \n",
      "4          -0.259688  Excel Data               150             Other   value5   \n",
      "\n",
      "   new_col2  \n",
      "0       1.0  \n",
      "1       2.0  \n",
      "2       3.0  \n",
      "3       4.0  \n",
      "4       5.0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "additional_columns = pd.DataFrame({\n",
    "    'new_col1': ['value1', 'value2', 'value3', 'value4', 'value5'],\n",
    "    'new_col2': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "df_concatenated_horizontal = pd.concat([df, additional_columns], axis=1)\n",
    "print(\"\\nDataFrame after horizontal concatenation:\")\n",
    "print(df_concatenated_horizontal.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5d96a-51bb-41d9-ba21-29834ddde626",
   "metadata": {},
   "source": [
    "### 25. Create a New Column Based on Existing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adfd7a9b-d8a3-4ef3-929b-c7e0d283e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after creating 'price_per_page' column:\n",
      "   price pages  price_per_page\n",
      "0    6.6   500        0.013200\n",
      "1   36.3   484        0.075000\n",
      "2   35.2   274        0.128467\n",
      "3   58.3   547        0.106581\n",
      "4   26.4   368        0.071739\n"
     ]
    }
   ],
   "source": [
    "df = df[pd.to_numeric(df['pages'], errors='coerce').notnull()]  # Drops rows with non-numeric pages\n",
    "df['price_per_page'] = df['price'] / df['pages'].astype(float)\n",
    "print(\"\\nDataFrame after creating 'price_per_page' column:\")\n",
    "print(df[['price', 'pages', 'price_per_page']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ce935-9c05-4cce-97bd-f29fd1634e17",
   "metadata": {},
   "source": [
    "### 26. Discretize a Continuous Column into Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b06f82f5-2601-41f0-8415-add5e018bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after discretizing 'price' column into bins:\n",
      "   price price_bins\n",
      "0    6.6       0-20\n",
      "1   36.3      20-40\n",
      "2   35.2      20-40\n",
      "3   58.3      40-60\n",
      "4   26.4      20-40\n"
     ]
    }
   ],
   "source": [
    "df['price_bins'] = pd.cut(df['price'], bins=[0, 20, 40, 60, 80, 100], labels=['0-20', '20-40', '40-60', '60-80', '80-100'])\n",
    "print(\"\\nDataFrame after discretizing 'price' column into bins:\")\n",
    "print(df[['price', 'price_bins']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb3461-fc3a-40d9-877c-69d3dfa67a90",
   "metadata": {},
   "source": [
    "### 27. Create Polynomial Features from Existing Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f8c07c9-8368-41d6-b696-ef46c96b22cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after creating polynomial features:\n",
      "   price  price_squared  avg_reviews  avg_reviews_squared\n",
      "0    6.6          43.56          4.4                19.36\n",
      "1   36.3        1317.69          4.3                18.49\n",
      "2   35.2        1239.04          4.7                22.09\n",
      "3   58.3        3398.89          4.6                21.16\n",
      "4   26.4         696.96          3.9                15.21\n"
     ]
    }
   ],
   "source": [
    "df['price_squared'] = df['price'] ** 2\n",
    "df['avg_reviews_squared'] = df['avg_reviews'] ** 2\n",
    "print(\"\\nDataFrame after creating polynomial features:\")\n",
    "print(df[['price', 'price_squared', 'avg_reviews', 'avg_reviews_squared']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
